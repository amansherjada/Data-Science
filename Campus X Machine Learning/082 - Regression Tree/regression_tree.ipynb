{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7aaae6a-2e88-475a-a752-10e8898212ed",
   "metadata": {},
   "source": [
    "## Regression Tree\n",
    "\n",
    "A regression tree is a type of decision tree used for solving regression problems. Instead of predicting discrete classes, like in classification, regression trees predict continuous values. They work by recursively partitioning the input space into smaller regions and assigning a numerical value to each region based on the average (or another summary statistic) of the target variable for the data points within that region.\r\n",
    "\r\n",
    "Here's a simplified explanation of how regression trees work:\r\n",
    "\r\n",
    "1. **Start with the Whole Dataset**: At the beginning, the entire dataset is considered one big region.\r\n",
    "\r\n",
    "2. **Select a Splitting Feature**: The algorithm looks for the feature and the threshold that best splits the dataset into two smaller regions. The goal is to minimize the variance of the target variable within each region.\r\n",
    "\r\n",
    "3. **Create Two Child Nodes**: Once the best split is found, the dataset is divided into two subsets based on the chosen feature and threshold.\r\n",
    "\r\n",
    "4. **Repeat the Process**: The process continues recursively for each subset, with the algorithm searching for the best split in each subset until a stopping criterion is met. This criterion could be reaching a maximum depth, having a minimum number of samples in each leaf node, or no further reduction in variance can be achieved.\r\n",
    "\r\n",
    "5. **Assign Predictions**: Finally, when the tree is fully grown, each terminal node (or leaf node) contains a prediction value. This value is typically the average of the target variable for the data points in that region.\r\n",
    "\r\n",
    "6. **Making Predictions**: To make predictions for new data points, you traverse the tree from the root node down to a leaf node based on the values of the features. Once you reach a leaf node, you use the prediction value associated with that node as the predicted output.\r\n",
    "\r\n",
    "In summary, regression trees partition the input space into smaller regions and assign a continuous value to each region based on the average of the target variable. They're intuitive and easy to interpret, making them useful for tasks where understanding the decision-making process is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3af73-85bb-40cf-9e62-66b59df6ff3a",
   "metadata": {},
   "source": [
    "#### Regression trees vs Linear regression\n",
    "\n",
    "![](https://i.stack.imgur.com/yE8JP.png)\n",
    "\n",
    "Regression trees can be advantageous over linear regression in certain scenarios due to their flexibility and ability to capture nonlinear relationships in the data. Here are some situations where regression trees might be preferred over linear regression:\r\n",
    "\r\n",
    "1. **Nonlinear Relationships**: When the relationship between the independent variables and the dependent variable is nonlinear, regression trees can capture these nonlinearities more effectively than linear regression. Linear regression assumes a linear relationship between the independent and dependent variables, while regression trees can model complex nonlinear relationships through recursive partitioning of the feature space.\r\n",
    "\r\n",
    "2. **Interaction Effects**: Regression trees can automatically capture interaction effects between variables without explicitly specifying them. In linear regression, you need to manually include interaction terms, which can become cumbersome and may not capture all interactions accurately.\r\n",
    "\r\n",
    "3. **Robustness to Outliers**: Regression trees are generally robust to outliers in the data. Outliers can significantly impact the coefficients estimated by linear regression models, leading to biased results. Regression trees, on the other hand, are less sensitive to outliers because they partition the data into regions and make predictions based on the majority of data points within each region.\r\n",
    "\r\n",
    "4. **Handling Categorical Variables**: Regression trees naturally handle categorical variables without requiring one-hot encoding or other preprocessing techniques. Linear regression typically requires categorical variables to be converted into dummy variables, which can increase the dimensionality of the feature space and potentially lead to overfitting.\r\n",
    "\r\n",
    "5. **Interpretability**: Regression trees offer intuitive interpretability, as the decision-making process can be visualized as a tree structure. This makes it easier to understand and explain the factors driving predictions compared to linear regression models, which may have coefficients that are harder to interpret, especially when dealing with interactions or multicollinearity.\r\n",
    "\r\n",
    "However, it's essential to consider the trade-offs when choosing between regression trees and linear regression. Regression trees can suffer from overfitting, especially if not properly regularized, and they may not perform well with small datasets. Additionally, they may not generalize as well to new data compared to linear regression, particularly if the relationships in the data are relatively simple and linear. Therefore, the choice between regression trees and linear regression should be based on the specific characteristics of the dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204405ee-4db1-4da2-afe5-42c9a872dfaa",
   "metadata": {},
   "source": [
    "### How Regression trees find splitting criteria?\n",
    "\n",
    "Regression trees find splitting criteria by selecting the feature and the threshold that minimizes some measure of impurity or variance within the resulting subsets. The process involves evaluating all possible splits on each feature and selecting the one that optimally separates the data into more homogeneous subsets.\n",
    "\n",
    "Here's a step-by-step explanation of how regression trees find splitting criteria:\n",
    "\n",
    "1. **Evaluate Potential Splits**: For each feature in the dataset, the algorithm evaluates all possible split points. For continuous features, potential split points are the unique values of that feature. For categorical features, each category is considered as a potential split point.\n",
    "\n",
    "2. **Calculate Impurity or Variance Reduction**: For each split point, the algorithm calculates a measure of impurity or variance within the resulting subsets. Common measures used for regression trees include:\n",
    "   - Mean Squared Error (MSE): Measures the average squared difference between the actual and predicted values within each subset.\n",
    "   - Total Variance Reduction: Measures the reduction in variance of the target variable achieved by the split.\n",
    "   \n",
    "3. **Select Best Split**: The algorithm selects the split point that maximizes the reduction in impurity or variance. This split point becomes the splitting criterion for that particular feature.\n",
    "\n",
    "4. **Repeat for All Features**: Steps 1-3 are repeated for all features in the dataset. The algorithm evaluates the potential splits for each feature and selects the best overall splitting criterion based on the chosen impurity or variance measure.\n",
    "\n",
    "5. **Choose the Best Split Among All Features**: Finally, the algorithm selects the feature and split point that result in the greatest reduction in impurity or variance across all features. This becomes the final splitting criterion used to partition the data into two subsets at that node of the tree.\n",
    "\n",
    "By iteratively selecting the feature and split point that minimizes impurity or variance, regression trees can recursively partition the input space into regions that lead to more accurate predictions of the target variable. This process continues until a stopping criterion is met, such as reaching a maximum tree depth or having all leaf nodes contain a minimum number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862eaee5-c1ab-498b-bc9d-45bb7d5a5e84",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor Hyperparameters\n",
    "\n",
    "```\n",
    "class sklearn.tree.DecisionTreeRegressor(*, criterion='squared_error', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0, monotonic_cst=None)\n",
    "```\n",
    "\n",
    "\r\n",
    "1. **criterion**:\r\n",
    "   - The criterion used to measure the quality of a split. For regression, it's typically 'squared_error', which minimizes the mean squared error (MSE) between the actual and predicted values.\r\n",
    "\r\n",
    "2. **splitter**:\r\n",
    "   - The strategy used to choose the split at each node. It can be 'best' to choose the best split or 'random' to choose the best random split.\r\n",
    "\r\n",
    "3. **max_depth**:\r\n",
    "   - The maximum depth of the tree. It limits the number of levels in the tree. Default is None, which means nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\r\n",
    "\r\n",
    "4. **min_samples_split**:\r\n",
    "   - The minimum number of samples required to split an internal node. Default is 2.\r\n",
    "\r\n",
    "5. **min_samples_leaf**:\r\n",
    "   - The minimum number of samples required to be at a leaf node. Default is 1.\r\n",
    "\r\n",
    "6. **min_weight_fraction_leaf**:\r\n",
    "   - The minimum weighted fraction of the sum total of weights required to be at a leaf node. Default is 0.0.\r\n",
    "\r\n",
    "7. **max_features**:\r\n",
    "   - The number of features to consider when looking for the best split. It can be an int, float, 'auto', 'sqrt', 'log2', None, or a fraction of features. Default is None.\r\n",
    "\r\n",
    "8. **random_state**:\r\n",
    "   - Seed used by the random number generator. It controls the randomness of the estimator. Default is None.\r\n",
    "\r\n",
    "9. **max_leaf_nodes**:\r\n",
    "   - The maximum number of leaf nodes in the tree. Default is None.\r\n",
    "\r\n",
    "10. **min_impurity_decrease**:\r\n",
    "    - A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default is 0.0.\r\n",
    "\r\n",
    "11. **ccp_alpha**:\r\n",
    "    - Complexity parameter used for Minimal Cost-Complexity Pruning. Default is 0.0.\r\n",
    "\r\n",
    "12. **monotonic_cst**:\r\n",
    "    - Monotonic constraints for the decision tree. Default is None.\r\n",
    "\r\n",
    "These hyperparameters control various aspects of the decision tree regressor, such as its depth, the criteria for splitting, and how it handles imbalanced data. By adjusting these parameters, you can tailor the decision tree model to suit the specific characteristics of your dataset and the requirements of your regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a9a5d-d5aa-4043-ba95-679c5c41a47c",
   "metadata": {},
   "source": [
    "### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5303c64-f32d-4847-b8f6-088007cf034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb0ab7a-f23b-45db-bbdd-c1829a5bce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771010f3-d8ef-479f-9f15-c8cf94876086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4      5     6       7    8      9     10  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "\n",
       "       11    12  \n",
       "0  396.90  4.98  \n",
       "1  396.90  9.14  \n",
       "2  392.83  4.03  \n",
       "3  394.63  2.94  \n",
       "4  396.90  5.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05316603-601d-49e5-811f-36b8739bbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = boston.feature_names\n",
    "df['MEDV'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc04192-333b-436a-9208-a5794f537b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6dbe37-07d3-4d08-a148-74f633b91542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4775a72d-19c7-4d22-adaf-b4fb565f93eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed24c1e3-2040-4737-8b70-0032a15b4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = DecisionTreeRegressor(criterion= 'squared_error', max_depth= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6abba5f1-2660-47a3-95e2-2e5a4964925e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252ac587-a5cc-44a6-b89e-aa8da6e8b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40afc0b6-ac8e-4d58-ae99-bd041d22ab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8833565347917997"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5c331-5c55-447e-ae51-42349aef39ea",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e78b36a-cd26-48ce-932b-97bbfed9dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth':[2,4,8,10,None],\n",
    "    'criterion':['mse','mae'],\n",
    "    'max_features':[0.25,0.5,1.0],\n",
    "    'min_samples_split':[0.25,0.5,1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11758249-8210-4df3-904b-02d280e48590",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GridSearchCV(DecisionTreeRegressor(), param_grid= param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b672cd3-de08-49c8-832c-cc9b08bd5cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=DecisionTreeRegressor(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;mse&#x27;, &#x27;mae&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [2, 4, 8, 10, None],\n",
       "                         &#x27;max_features&#x27;: [0.25, 0.5, 1.0],\n",
       "                         &#x27;min_samples_split&#x27;: [0.25, 0.5, 1.0]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=DecisionTreeRegressor(),\n",
       "             param_grid={&#x27;criterion&#x27;: [&#x27;mse&#x27;, &#x27;mae&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [2, 4, 8, 10, None],\n",
       "                         &#x27;max_features&#x27;: [0.25, 0.5, 1.0],\n",
       "                         &#x27;min_samples_split&#x27;: [0.25, 0.5, 1.0]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeRegressor(),\n",
       "             param_grid={'criterion': ['mse', 'mae'],\n",
       "                         'max_depth': [2, 4, 8, 10, None],\n",
       "                         'max_features': [0.25, 0.5, 1.0],\n",
       "                         'min_samples_split': [0.25, 0.5, 1.0]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdca6c6e-a1af-499f-afd1-c4c684809792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6254023352956153"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71368965-3fa1-41ed-934a-5c727403961b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'mse',\n",
       " 'max_depth': 4,\n",
       " 'max_features': 1.0,\n",
       " 'min_samples_split': 0.25}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5fe00-7111-4c9e-8826-a714d951d229",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a6bb36b-f0cd-41a7-a189-67cc6e77b025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RM 0.6292222501550527\n",
      "LSTAT 0.20562720153418312\n",
      "DIS 0.07272221949122784\n",
      "CRIM 0.05725882102630604\n",
      "TAX 0.01669708628286318\n",
      "AGE 0.00617612617436646\n",
      "PTRATIO 0.0055650568587021785\n",
      "NOX 0.0035610403857025503\n",
      "INDUS 0.0026274687266826754\n",
      "B 0.0005427293649131192\n",
      "ZN 0.0\n",
      "RAD 0.0\n",
      "CHAS 0.0\n"
     ]
    }
   ],
   "source": [
    "for importance, name in sorted(zip(rt.feature_importances_, X_train.columns),reverse=True):\n",
    "  print (name, importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b46a5-8e68-4056-97a2-78e16cd59cea",
   "metadata": {},
   "source": [
    "In a decision tree model, `rt.feature_importances_` refers to an attribute that holds the importance scores of each feature in the dataset as determined by the trained decision tree model `rt`.\n",
    "\n",
    "The feature importance represents the relative importance of each feature in making accurate predictions with the decision tree model. It is calculated based on how much each feature contributes to reducing impurity (e.g., Gini impurity or entropy) at each node of the decision tree during the training process.\n",
    "\n",
    "Higher feature importance values indicate that the feature is more influential in making decisions within the decision tree model. These scores can provide insights into which features are most informative or relevant for the model's predictions.\n",
    "\n",
    "In the code snippet provided earlier, `rt.feature_importances_` is used to obtain the feature importance scores, which are then zipped with the column names of the features (`X_train.columns`). This allows for sorting and printing the features along with their corresponding importance scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
