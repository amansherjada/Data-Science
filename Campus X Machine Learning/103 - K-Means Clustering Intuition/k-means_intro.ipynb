{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfe6e23-3b50-4079-bec5-97a326fd24df",
   "metadata": {},
   "source": [
    "Explanation = https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning\n",
    "\n",
    "![k-means](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4e8f2-6c4b-4ce4-bdf4-6c3e2d8c8d7c",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "\n",
    "K-means clustering is a popular unsupervised learning algorithm used for partitioning a dataset into K distinct, non-overlapping clusters. The algorithm iteratively assigns each data point to one of K clusters based on the feature similarity and then computes the centroids of these clusters. Here's the geometric intuition behind how K-means works:\r\n",
    "\r\n",
    "1. **Initialization**: The algorithm starts by randomly initializing K cluster centroids. These centroids serve as the initial points around which the clusters will form.\r\n",
    "\r\n",
    "2. **Assignment Step**: For each data point in the dataset, calculate the distance (commonly Euclidean distance) between that point and each of the K centroids. Assign the data point to the cluster represented by the nearest centroid. This step partitions the data into K clusters.\r\n",
    "\r\n",
    "3. **Update Step**: After all data points have been assigned to clusters, compute the new centroids for each cluster by taking the mean of all data points assigned to that cluster. These new centroids represent the \"center of mass\" for each cluster.\r\n",
    "\r\n",
    "4. **Iterate**: Repeat the assignment and update steps iteratively until convergence criteria are met. Convergence usually occurs when the centroids no longer change significantly between iterations or when a maximum number of iterations is reached.\r\n",
    "\r\n",
    "The geometric intuition behind these steps can be visualized as follows:\r\n",
    "\r\n",
    "- **Clusters as Groups**: Think of each cluster as a group of data points that are close to each other in the feature space. The goal is to group similar data points together, forming compact clusters.\r\n",
    "\r\n",
    "- **Centroids as Centers**: The centroids represent the \"centers\" of these clusters. They are calculated as the average of all data points within each cluster, so they lie at the geometric center of the cluster in the feature space.\r\n",
    "\r\n",
    "- **Minimizing Intra-cluster Variance**: The objective of K-means is to minimize the sum of squared distances between data points and their respective cluster centroids. This is equivalent to minimizing the intra-cluster variance, ensuring that data points within each cluster are tightly packed around their centroid.\r\n",
    "\r\n",
    "- **Partitioning Space**: K-means effectively partitions the feature space into K regions, with each region corresponding to one cluster. Data points are assigned to the cluster whose centroid is closest to them, resulting in a Voronoi partitioning of the space.\r\n",
    "\r\n",
    "- **Iterative Refinement**: Through successive iterations, K-means refines the cluster centroids to better represent the data distribution. As centroids are updated, data points may be reassigned to different clusters if a closer centroid emerges.\r\n",
    "\r\n",
    "- **Convergence**: Eventually, the algorithm converges to a stable configuration where further iterations do not significantly change the centroids or cluster assignments. At this point, the algorithm has found a locally optimal solution.\r\n",
    "\r\n",
    "Overall, K-means clustering can be understood geometrically as an iterative process of partitioning the feature space into compact, well-separated clusters, with centroids representing the centers of these clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5617c-ba03-46ec-9ab5-b4c2982f6b9c",
   "metadata": {},
   "source": [
    "### Elbow method\n",
    "\n",
    "![Elbow](https://www.researchgate.net/publication/339823520/figure/fig3/AS:867521741733888@1583844709013/The-elbow-method-of-k-means.png)\n",
    "\n",
    "The elbow method is a technique used to determine the optimal number of clusters (K) in a dataset for K-means clustering. It involves running the K-means algorithm for a range of K values and plotting the within-cluster sum of squares (WCSS) against the number of clusters. The \"elbow\" point in the plot represents the optimal number of clusters, where the rate of decrease in WCSS slows down significantly. Here's how it works:\n",
    "\n",
    "1. **Choose a Range of K**: First, you need to decide on a range of possible values for K. This could start from K=1 and extend up to a reasonable maximum value based on your understanding of the data and domain knowledge.\n",
    "\n",
    "2. **Run K-means for Each K**: Apply the K-means algorithm to the dataset for each value of K in the chosen range. Each run of K-means will produce a set of cluster centroids and the corresponding WCSS.\n",
    "\n",
    "3. **Calculate WCSS**: For each value of K, compute the sum of squared distances between each data point and its nearest cluster centroid. This gives you the WCSS for that value of K.\n",
    "\n",
    "4. **Plot WCSS vs. K**: Plot the number of clusters (K) on the x-axis and the corresponding WCSS on the y-axis.\n",
    "\n",
    "5. **Identify the Elbow Point**: Look for the point in the plot where the rate of decrease in WCSS slows down significantly. This point resembles an elbow in the plot. The elbow point represents the optimal number of clusters, as adding more clusters beyond this point does not lead to a significant improvement in clustering quality.\n",
    "\n",
    "6. **Choose the Optimal K**: Based on the elbow point in the plot, select the optimal number of clusters for your dataset. This K value can then be used to perform the final K-means clustering.\n",
    "\n",
    "The intuition behind the elbow method is that as the number of clusters increases, the WCSS tends to decrease because each cluster will have fewer data points, resulting in smaller distances between points and their centroids. However, beyond a certain point, adding more clusters does not lead to a significant reduction in WCSS, indicating that additional clusters may be capturing noise or irrelevant patterns rather than meaningful structure in the data.\n",
    "\n",
    "By identifying the elbow point in the plot, you can strike a balance between the complexity of the clustering solution (number of clusters) and the quality of the clustering (compactness of clusters), thereby finding a suitable trade-off for your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7233984-d22f-4726-aa7f-20c15a095dbd",
   "metadata": {},
   "source": [
    "### K-means clustering algorithm steps\r\n",
    "\r\n",
    "1. **Initialization**: \r\n",
    "   - Choose the number of clusters (K) you want to find.\r\n",
    "   - Randomly pick K points in your dataset. These points will be the initial centroids of the clusters.\r\n",
    "\r\n",
    "2. **Assignment Step**:\r\n",
    "   - For each data point in your dataset, calculate the distance to each centroid.\r\n",
    "   - Assign each data point to the nearest centroid. This groups the points into K clusters.\r\n",
    "\r\n",
    "3. **Update Step**:\r\n",
    "   - After all data points are assigned to clusters, calculate the new centroid of each cluster by taking the mean of all points assigned to that cluster.\r\n",
    "\r\n",
    "4. **Repeat**:\r\n",
    "   - Keep repeating the assignment and update steps until the centroids stop moving significantly or until a maximum number of iterations is reached.\r\n",
    "\r\n",
    "5. **Result**:\r\n",
    "   - Once the algorithm converges (i.e., centroids stop changing), you have your clusters.\r\n",
    "   - Each data point is now assigned to one of the K clusters based on its nearest centroid.\r\n",
    "\r\n",
    "6. **Finalization**:\r\n",
    "   - You can now analyze and interpret your clusters. Each cluster represents a group of data points that are similar to each other.\r\n",
    "   - Optionally, you can use the centroids as representatives of each cluster, which can be useful for making predictions or understanding the characteristics of each cluster.\r\n",
    "\r\n",
    "In essence, K-means clustering is about repeatedly refining the positions of the centroids and reassigning data points to the nearest centroid until the clusters stabilize. The algorithm aims to minimize the distance between data points and their respective centroids, resulting in well-defined clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2f990-a28c-4a8a-9e89-492c0dc01466",
   "metadata": {},
   "source": [
    "### K-means clustering algorithm into simple steps:\n",
    "\n",
    "![1](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning2.png)\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start by deciding how many clusters (K) you want to create.\n",
    "   - Randomly choose K points from your dataset. These points will act as the initial centroids for your clusters.\n",
    "\n",
    "![2](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning3.png)\n",
    "\n",
    "2. **Assignment Step**:\n",
    "   - For each data point in your dataset, calculate its distance to each centroid.\n",
    "   - Assign each data point to the cluster represented by the nearest centroid. This means each data point belongs to the cluster whose centroid it's closest to.\n",
    "\n",
    "![3](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning4.png)\n",
    "\n",
    "3. **Update Step**:\n",
    "   - After assigning all data points to clusters, recalculate the centroids of the clusters.\n",
    "   - Each centroid is recalculated as the mean of all data points assigned to its cluster. This moves the centroids to the center of their respective clusters.\n",
    "\n",
    "![4](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning6.png)\n",
    "\n",
    "4. **Iteration**:\n",
    "   - Repeat the assignment and update steps until one of the stopping conditions is met:\n",
    "     - Centroids stop moving significantly (convergence).\n",
    "     - Maximum number of iterations is reached.\n",
    "\n",
    "![5](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning9.png)\n",
    "\n",
    "5. **Convergence**:\n",
    "   - Once the algorithm stops because centroids have stabilized, you have your clusters.\n",
    "\n",
    "![6](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning10.png)\n",
    "\n",
    "6. **Result**:\n",
    "   - Each data point now belongs to one of the K clusters.\n",
    "   - You can analyze these clusters to understand patterns in your data.\n",
    "\n",
    "![7](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning11.png)\n",
    "\n",
    "7. **Interpretation**:\n",
    "   - Each cluster represents a group of data points that are similar to each other and dissimilar to data points in other clusters.\n",
    "   - You can interpret the characteristics of each cluster by examining the data points it contains and the centroid that represents it.\n",
    "\n",
    "![8](https://static.javatpoint.com/tutorial/machine-learning/images/k-means-clustering-algorithm-in-machine-learning12.png)\n",
    "\n",
    "In essence, K-means clustering is about iteratively refining the positions of the cluster centers (centroids) and reassigning data points to clusters until the centroids stabilize and the clusters become well-defined. The algorithm aims to minimize the distance between data points and their respective centroids, resulting in tight, cohesive clusters that capture meaningful patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
