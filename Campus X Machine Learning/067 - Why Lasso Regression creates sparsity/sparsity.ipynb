{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dab11b0-99b0-40d5-a8ac-3fc8b2f9d075",
   "metadata": {},
   "source": [
    "### Sparsity\n",
    "\n",
    "Sparsity refers to the property of having many elements equal to zero. In the context of machine learning and regression, sparsity often refers to having many coefficients of a model equal to zero.\n",
    "\n",
    "Lasso Regression creates sparsity through its regularization mechanism. Here's why:\n",
    "\n",
    "1. **L1 Regularization (Lasso):**\n",
    "   - Lasso Regression adds an L1 penalty to the loss function, proportional to the absolute sum of the coefficients.\n",
    "   - The penalty term in Lasso Regression is `alpha * sum(abs(w_i))`, where alpha is the regularization parameter, w_i are the individual coefficients, and n is the total number of coefficients.\n",
    "   - This penalty term encourages the coefficients to be as small as possible while still fitting the data well. However, because of the absolute value in the penalty term, it tends to force some coefficients to exactly zero.\n",
    "\n",
    "2. **Effect on Coefficients:**\n",
    "   - As the regularization parameter alpha increases, the penalty on the coefficients becomes stronger.\n",
    "   - Some coefficients will be reduced to zero faster than others, especially those associated with less relevant or redundant features.\n",
    "   - Eventually, as alpha increases further, more coefficients are driven to zero until only a subset of the original features remains with non-zero coefficients.\n",
    "\n",
    "3. **Feature Selection:**\n",
    "   - The process of driving coefficients to zero effectively performs feature selection. Features associated with zero coefficients are effectively ignored by the model, as their impact on the predictions becomes negligible.\n",
    "   - This feature selection capability of Lasso Regression is particularly useful in situations where there are many features, some of which may be irrelevant or redundant. It helps simplify the model and improve interpretability by focusing on the most important features.\n",
    "\n",
    "In summary, Lasso Regression creates sparsity by inducing some coefficients to be exactly zero through its L1 regularization penalty. This feature selection property makes Lasso Regression particularly effective in situations with high-dimensional data where feature selection or dimensionality reduction is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8e755-efaf-4e2d-989d-8d1421f42c6f",
   "metadata": {},
   "source": [
    "### Simple Explaination\n",
    "\n",
    "Sparsity refers to the property of having a lot of zeros in a dataset or model. In the context of machine learning and regression models, sparsity means that many of the coefficients (or weights) associated with the input features are zero.\n",
    "\n",
    "Lasso Regression creates sparsity because of the way it penalizes the coefficients during training:\n",
    "\n",
    "1. **Lasso Penalty:**\n",
    "   - Lasso Regression adds a penalty to the coefficient values based on the sum of their absolute magnitudes (L1 penalty).\n",
    "   - This penalty encourages the model to simplify itself by setting some coefficients to zero.\n",
    "  \n",
    "2. **Feature Selection:**\n",
    "   - As Lasso Regression optimizes the model during training, it tends to drive less important coefficients down to zero more aggressively.\n",
    "   - In other words, features that are not very useful for predicting the target variable may end up with zero coefficients.\n",
    "   - This effectively performs feature selection, as only the most important features with non-zero coefficients remain in the model.\n",
    "\n",
    "So, in simple terms, sparsity means having a lot of zeros. Lasso Regression creates sparsity by penalizing the coefficients in such a way that less important features end up with zero coefficients, effectively removing them from the model. This helps in simplifying the model and improving its interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee93949-03c5-4f3b-8751-1078df8ab6a1",
   "metadata": {},
   "source": [
    "![Formula](lasso_m_formula.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b4956-8af2-4a9c-ac04-3857272fb0be",
   "metadata": {},
   "source": [
    "### Why Lasso Regression stops at zero?\n",
    "\n",
    "Lasso Regression, also known as L1 regularization, includes a penalty term in the loss function that penalizes the absolute size of the coefficients of the regression model. This penalty term is proportional to the sum of the absolute values of the coefficients. \n",
    "\n",
    "The effect of this penalty is to shrink the coefficients towards zero, potentially causing some coefficients to become exactly zero. When a coefficient becomes zero, it means that the corresponding feature is effectively excluded from the model. This is a form of feature selection, where Lasso Regression automatically selects a subset of the most important features by setting the coefficients of less important features to zero.\n",
    "\n",
    "Mathematically, the reason Lasso Regression tends to force some coefficients to exactly zero lies in the nature of the L1 penalty term. The optimization process used to minimize the loss function with the L1 penalty term often leads to sparse solutions, where some coefficients are exactly zero. This is in contrast to Ridge Regression (L2 regularization), which tends to shrink the coefficients towards zero but rarely results in exactly zero coefficients.\n",
    "\n",
    "So, in summary, Lasso Regression stops at zero because of the L1 penalty term in the loss function, which promotes sparsity and feature selection by setting some coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e6717-40f9-4f4d-8051-433482a965ab",
   "metadata": {},
   "source": [
    "Reference = https://www.pythonkitchen.com/lasso-sparsity/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
