{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ce26bc-d8d4-4f3f-9726-916ebac27d13",
   "metadata": {},
   "source": [
    "## ElasticNet Regression\n",
    "\n",
    "ElasticNet Regression is a type of regression technique that combines the properties of both Lasso Regression (L1 regularization) and Ridge Regression (L2 regularization). \r\n",
    "\r\n",
    "In simple terms, ElasticNet Regression is like a blend of Lasso and Ridge Regression. It uses a penalty term that includes both the L1 and L2 norms of the coefficients.\r\n",
    "\r\n",
    "Here's a breakdown of what it does:\r\n",
    "\r\n",
    "1. **L1 Regularization (Lasso):** Promotes sparsity by forcing some coefficients to exactly zero, thus performing feature selection.\r\n",
    "\r\n",
    "2. **L2 Regularization (Ridge):** Prevents overfitting by shrinking the coefficients towards zero without necessarily setting them exactly to zero.\r\n",
    "\r\n",
    "ElasticNet Regression combines these two regularization techniques by including both L1 and L2 penalty terms in the loss function. This allows it to address some of the limitations of Lasso and Ridge Regression individually. \r\n",
    "\r\n",
    "By adjusting the parameters controlling the balance between L1 and L2 regularization, ElasticNet Regression can effectively handle situations where there are highly correlated predictors (features) and where there are groups of correlated predictors. This makes it more flexible and potentially more powerful in certain situations compared to Lasso or Ridge Regression alone. \r\n",
    "\r\n",
    "In summary, ElasticNet Regression is a versatile regression technique that combines the strengths of Lasso and Ridge Regression to provide a more robust and flexible approach to regression analysis, particularly when dealing with high-dimensional data with correlated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b483e-0632-4175-817a-745cf49dd51e",
   "metadata": {},
   "source": [
    "![Formula](elasticnetformula.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156564dd-4d58-484b-a5a5-7fb01a8ccb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b070fe1-cda1-4313-a78d-0cd722d305d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8860626e-1400-41b0-b211-ba17d3411b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bef4739-caac-41be-be4b-00ce8d6e2921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399338661568968"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6fa6c0-2a9b-46e0-96c7-72bf51c0efe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45199494197195456"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge \n",
    "reg = Ridge(alpha=0.1)\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b95c12d-05d4-49b7-8c87-a3a206407631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44111855963110613"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso\n",
    "reg = Lasso(alpha=0.01)\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6420e310-5a30-443f-87ff-81e4151e042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4531474541554823"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ElasticNet\n",
    "reg = ElasticNet(alpha=0.005,l1_ratio=0.9)\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ad196-eda8-45a2-84e7-03264cf88b83",
   "metadata": {},
   "source": [
    "### When to use Lasso, Ridge, and ElasticNet\n",
    "\n",
    "Choosing between Lasso, Ridge, and ElasticNet Regression depends on the characteristics of your dataset and the goals of your analysis. Here's a guideline for when to use each:\n",
    "\n",
    "1. **Lasso Regression (L1 regularization):**\n",
    "   - Use Lasso Regression when you suspect that many of your features are irrelevant or redundant, and you want to perform feature selection by forcing some coefficients to exactly zero.\n",
    "   - Lasso is particularly useful when dealing with high-dimensional datasets where there are more features than observations, as it helps to reduce overfitting and improve model interpretability by selecting the most important features.\n",
    "   - Use when we know that not every columns are important\n",
    "\n",
    "2. **Ridge Regression (L2 regularization):**\n",
    "   - Use Ridge Regression when you have many predictors that are correlated with each other, and you want to shrink the coefficients towards zero without necessarily setting them exactly to zero.\n",
    "   - Ridge Regression is effective at reducing the impact of multicollinearity and stabilizing the model by preventing the coefficients from becoming too large.\n",
    "   - Use when we know that every columns are important\n",
    "\n",
    "3. **ElasticNet Regression:**\n",
    "   - Use ElasticNet Regression when you have a dataset with both high-dimensional features and multicollinearity.\n",
    "   - ElasticNet combines the benefits of both Lasso and Ridge Regression, allowing for feature selection while also handling correlated predictors more effectively.\n",
    "   - It's particularly useful when Lasso alone may be too aggressive in feature selection due to high multicollinearity, or when Ridge alone may not be sufficient in reducing the number of features.\n",
    "   - Use when we don't know that columns are important or not.\n",
    "\n",
    "In summary:\n",
    "- Use Lasso for feature selection and when you expect a sparse solution.\n",
    "- Use Ridge to deal with multicollinearity and when you want to stabilize the model.\n",
    "- Use ElasticNet when you have both high-dimensional data and multicollinearity, or when you want a balanced approach between feature selection and coefficient shrinkage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
