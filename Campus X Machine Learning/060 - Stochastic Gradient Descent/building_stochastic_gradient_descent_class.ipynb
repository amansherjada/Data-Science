{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42aa1bc0-498a-4fc3-be50-9e2c283f5573",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a variant of the gradient descent optimization algorithm commonly used in machine learning for training models, particularly in scenarios where the dataset is large. \r\n",
    "\r\n",
    "Here'sle explanation of how it works:\r\n",
    "\r\n",
    "1. **Gradient Descent**: In traditional gradient descent (also known as batch gradient descent), you compute the gradient of the cost function with respect to the parameters for the entire dataset. Then, you update the parameters once based on this average gradient.\r\n",
    "\r\n",
    "2. **Stochastic Gradient Descent**: In SGD, instead of computing the gradient over the entire dataset, you randomly pick a single data point (or a small subset, called a mini-batch) from the dataset. You compute the gradient of the cost function with respect to the parameters using only that single data point (or mini-batch), and then you update the parameters. This process is repeated for each data point (or mini-batch) in the dataset.\r\n",
    "\r\n",
    "Advantages of Stochastic Gradient Descent over Batch Gradient Descent:\r\n",
    "\r\n",
    "1. **Efficiency**: SGD is often faster because it updates the parameters more frequently. With each update, it takes a step in the direction that minimizes the cost function, potentially converging to the minimum more quickly.\r\n",
    "\r\n",
    "2. **Less Memory Requirement**: Since SGD only requires calculating the gradient for a single data point (or a small subset), it consumes much less memory compared to batch gradient descent, making it more suitable for large datasets that cannot fit into memory.\r\n",
    "\r\n",
    "3. **Possibly Better Generalization**: SGD's frequent updates and exposure to individual data points (or mini-batches) can introduce more randomness into the optimization process, potentially helping the algorithm to escape local minima and find better solutions or generalize better to unseen data.\r\n",
    "\r\n",
    "However, SGD can also have some drawbacks, such as more frequent fluctuations in the objective function and slower convergence towards the minimum when the cost surface is not smooth. To mitigate these issues, techniques like learning rate scheduling, momentum, and adaptive learning rates are often employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce9f2a0-dcb9-47b5-a338-ce360a745b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbe7e22-c4ca-4ba5-9369-878332f2b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4dd94c-01ac-4593-9191-29c3e98c45a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f99515a-0e7a-415c-847f-58f485840f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78883633-099b-4391-9462-5fc4f21e15e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00188202, -0.04464164, -0.06979687, ..., -0.03949338,\n",
       "        -0.06291688,  0.04034337],\n",
       "       [-0.00914709, -0.04464164,  0.01103904, ..., -0.03949338,\n",
       "         0.01703607, -0.0052198 ],\n",
       "       [ 0.02354575,  0.05068012, -0.02021751, ..., -0.03949338,\n",
       "        -0.09643495, -0.01764613],\n",
       "       ...,\n",
       "       [ 0.06350368,  0.05068012, -0.00405033, ..., -0.00259226,\n",
       "         0.08449153, -0.01764613],\n",
       "       [-0.05273755,  0.05068012, -0.01806189, ...,  0.1081111 ,\n",
       "         0.03606033, -0.04249877],\n",
       "       [ 0.00175052,  0.05068012,  0.05954058, ...,  0.1081111 ,\n",
       "         0.06898589,  0.12732762]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d8d52d-8cea-4e10-b712-cfbad830f352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5990cdb5-6d9b-403e-9f11-49fb173c804b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399338661568968"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346c315a-8945-489a-81cc-0df48ea674b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -9.15865318, -205.45432163,  516.69374454,  340.61999905,\n",
       "       -895.5520019 ,  561.22067904,  153.89310954,  126.73139688,\n",
       "        861.12700152,   52.42112238])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fe53c2-4dfd-4823-bef5-510b811c625d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.88331005254167"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4131e8b5-5869-430e-8c00-3ebbdea2376f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2beab3bb-3dd1-4d2b-9440-ef2d9f5c7b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77e72f7c-35d2-45ba-b5cc-fe34abfaf637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420ba361-74c6-4f06-b2bf-b214c7605a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class amanSGD:\n",
    "    def __init__(self, learning_rate = 0.1, epochs = 100):\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(X_train.shape[0]):\n",
    "                idx = np.random.randint(0, X_train.shape[0])\n",
    "                y_hat = np.dot(X_train[idx], self.coef_) + self.intercept_\n",
    "                # in sgd the formula will be -2 (yi - y_hat) because we are not calculating the whole derivative\n",
    "                intercept_der = -2 * (y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat), X_train[idx])\n",
    "                self.coef_ = self.coef_ -(self.lr * coef_der)\n",
    "        print(self.intercept_, self.coef_)\n",
    "    def predict(self, X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52eee43a-8cbc-4841-b168-8ee56d69f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = amanSGD(learning_rate=0.01, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58adb5f-5cb5-46f3-90da-9e90f4982b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145.94400660033554 [  56.16314182  -76.06313579  355.90336265  245.72488747   18.23406522\n",
      "  -26.70496554 -169.42389724  126.89187121  311.14697342  130.73297417]\n"
     ]
    }
   ],
   "source": [
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a11e22e-3a2b-4e31-ac8e-7f2ad9766077",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0742c1e1-677f-4ced-a534-41f229d056ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4283703691763032"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b208f3-3f12-486a-a166-5716494126de",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD) disadvantages:\r\n",
    "\r\n",
    "1. **Noisy Updates**: Because SGD updates parameters based on the gradient computed from a single data point (or a small subset), the updates can be very noisy, leading to a more erratic convergence path compared to batch gradient descent. This noise can sometimes slow down convergence or make it harder to converge to an optimal solution.\r\n",
    "\r\n",
    "2. **Variance in Convergence**: Due to the randomness introduced by sampling individual data points (or mini-batches), SGD may exhibit higher variance in convergence behavior compared to batch gradient descent. This variance can make it more challenging to determine the convergence criteria or to reproduce the same results across different runs.\r\n",
    "\r\n",
    "3. **Sensitive to Learning Rate**: The learning rate in SGD needs to be carefully tuned. If the learning rate is too high, SGD may oscillate around the minimum or even diverge. If it's too low, convergence may be slow. Finding an appropriate learning rate can be more challenging in SGD compared to batch gradient descent.\r\n",
    "\r\n",
    "4. **Potential for Plateaus**: In certain cases, especially when the cost surface is relatively flat or has long, shallow plateaus, SGD may struggle to make progress towards the minimum because the gradient from individual data points (or mini-batches) may not provide sufficient guidance.\r\n",
    "\r\n",
    "5. **Difficulty in Diagnosing Convergence**: Because of the stochastic nature of SGD updates, diagnosing convergence issues can be more challenging compared to batch gradient descent. It may require additional monitoring techniques or multiple runs to ensure convergence to a satisfactory solution.\r\n",
    "\r\n",
    "Despite these disadvantages, SGD remains widely used due to its efficiency, scalability to large datasets, and ability to handle non-convex optimization problems. Various techniques, such as momentum, adaptive learning rates, and mini-batch sampling strategies, are often employed to mitigate these issues and improve the performance of SGD in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec24ce6-811f-40c5-b797-dcb138b55162",
   "metadata": {},
   "source": [
    "### When to use Stochastic Gradient Descent (SGD) and Batch Gradient Descent\n",
    "\n",
    "Choosing between Stochastic Gradient Descent (SGD) and Batch Gradient Descent depends on various factors such as the size of the dataset, computational resources, and optimization goals. Here's a guideline on when to use each:\n",
    "\n",
    "1. **Batch Gradient Descent (BGD)**:\n",
    "   - **Small to Medium Sized Datasets**: BGD is suitable when the dataset can comfortably fit into memory.\n",
    "   - **Smooth Cost Functions**: BGD may perform well when dealing with smooth, well-behaved cost functions, as it computes the gradient over the entire dataset, leading to more stable updates.\n",
    "   - **Convergence**: If convergence speed is crucial and computational resources are not a limitation, BGD might be preferred as it guarantees a monotonic decrease in the cost function with each iteration.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**:\n",
    "   - **Large Datasets**: SGD is well-suited for large datasets that cannot fit into memory because it updates parameters based on individual data points (or mini-batches), requiring less memory.\n",
    "   - **Efficiency**: When computational resources are limited, SGD is often preferred due to its computational efficiency. It allows for more frequent updates, potentially converging faster, especially in high-dimensional spaces.\n",
    "   - **Non-convex Optimization**: SGD can be beneficial in non-convex optimization problems, as its stochastic updates can help escape local minima and explore the solution space more effectively.\n",
    "   - **Online Learning**: For scenarios where new data arrives continuously, SGD is suitable for online learning settings, where the model is updated incrementally with each new observation.\n",
    "\n",
    "3. **Mini-Batch Gradient Descent**:\n",
    "   - **Trade-off between BGD and SGD**: Mini-batch gradient descent combines aspects of both BGD and SGD by updating parameters based on small random subsets (mini-batches) of the dataset. It strikes a balance between the efficiency of SGD and the stability of BGD, making it suitable for a wide range of scenarios.\n",
    "   - **Parallelization**: Mini-batch gradient descent can also be parallelized across multiple processing units, making it useful in distributed computing environments.\n",
    "\n",
    "In practice, the choice between BGD, SGD, or mini-batch gradient descent often depends on experimentation and empirical evaluation, considering factors such as dataset size, computational constraints, convergence behavior, and optimization objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313d7dd-0498-4433-a1f3-5a086d78e3d0",
   "metadata": {},
   "source": [
    "### Convex function and Non Convex function\n",
    "\n",
    "![](https://d3i71xaburhd42.cloudfront.net/b1df0cd796034ca9ba3bc018474e44ee60fd7855/21-Figure1.5-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a741dbe-0871-47c0-b2da-2be59849675c",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*QA0kOv7KA_0SNWaEEHCmZg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8827f33-3399-4fb4-a78a-f99be3ed75dc",
   "metadata": {},
   "source": [
    "### Sklearn SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77326102-c076-4cbf-9cbc-33761b81cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e888326b-0d59-47ad-b39a-fe2a7aff7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = SGDRegressor(max_iter=100, learning_rate='constant', eta0=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9cc8591-46db-4dc1-8fac-e2628f4d2524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sher Mohammed Khan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(learning_rate=&#x27;constant&#x27;, max_iter=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(learning_rate=&#x27;constant&#x27;, max_iter=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor(learning_rate='constant', max_iter=100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01f8b855-fb3c-40c2-8faf-c36329834dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "221eee84-d5be-4cb1-baba-60c264dd7045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43066316900537893"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
