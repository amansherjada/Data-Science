{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2001e0-98e1-49c0-bebf-26cf87027f9c",
   "metadata": {},
   "source": [
    "## Bagging vs Boosting\n",
    "\n",
    "![bvb](https://images.datacamp.com/image/upload/v1700592126/image1_fcace6f2b3.png)\n",
    "\n",
    "Bagging and boosting are both ensemble learning techniques used to improve the performance of machine learning models by combining multiple base learners. However, they differ in their approaches and how they leverage the base learners. Here's a comparison of bagging vs. boosting:\r\n",
    "\r\n",
    "1. **Bagging (Bootstrap Aggregating)**:\r\n",
    "   - Bagging involves training multiple base learners independently on different subsets of the training data, sampled with replacement (bootstrap samples).\r\n",
    "   - Each base learner learns from a random subset of the data, which introduces diversity among the learners.\r\n",
    "   - The final prediction is obtained by averaging (for regression) or voting (for classification) the predictions of all base learners.\r\n",
    "   - Bagging helps reduce variance and overfitting, especially when the base learners tend to overfit the training data.\r\n",
    "   - Random Forest is a popular ensemble method based on bagging, where the base learners are decision trees.\r\n",
    "\r\n",
    "2. **Boosting**:\r\n",
    "   - Boosting involves training multiple base learners sequentially, where each learner corrects the errors made by its predecessors.\r\n",
    "   - Each base learner is trained on a modified version of the training data, where the weights of misclassified instances are adjusted to focus more on them.\r\n",
    "   - The final prediction is a weighted combination of the predictions of all base learners, where the weights are determined based on the performance of each learner.\r\n",
    "   - Boosting aims to reduce bias and improve model performance by focusing on difficult-to-classify instances.\r\n",
    "   - AdaBoost and Gradient Boosting Machines (GBM) are popular boosting algorithms.\r\n",
    "\r\n",
    "**Main Differences**:\r\n",
    "\r\n",
    "- **Approach**: Bagging trains base learners independently, while boosting trains them sequentially.\r\n",
    "- **Sampling**: Bagging samples data with replacement (bootstrap sampling), while boosting modifies the weights of training instances.\r\n",
    "- **Base Learners**: In bagging, base learners can be any type of model, while boosting typically uses weak learners (models that perform slightly better than random guessing).\r\n",
    "- **Error Correction**: Boosting focuses on correcting errors made by previous learners, whereas bagging does not explicitly correct errors.\r\n",
    "\r\n",
    "**Choosing Between Bagging and Boosting**:\r\n",
    "\r\n",
    "- Use **bagging**:\r\n",
    "  - When working with high-variance models prone to overfitting.\r\n",
    "  - When the goal is to reduce variance and improve stability.\r\n",
    "  - When computational resources are limited, as bagging can be parallelized.\r\n",
    "\r\n",
    "- Use **boosting**:\r\n",
    "  - When working with high-bias models that underfit the data.\r\n",
    "  - When aiming to reduce bias and improve accuracy.\r\n",
    "  - When willing to trade off some computational resources for potentially better performance. Boosting typically requires more computation than bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b18a99-eaea-43f7-9c6b-9779cb354bca",
   "metadata": {},
   "source": [
    "\r\n",
    "| Aspect                  | Bagging                                      | Boosting                                     |\r\n",
    "|-------------------------|----------------------------------------------|----------------------------------------------|\r\n",
    "| Approach                | Trains base learners independently           | Trains base learners sequentially            |\r\n",
    "| Sampling                | Samples data with replacement (bootstrap)     | Adjusts instance weights for training        |\r\n",
    "| Base Learners           | Can be any type of model                     | Typically uses weak learners                |\r\n",
    "| Error Correction        | Does not explicitly correct errors            | Focuses on correcting errors sequentially   |\r\n",
    "| Main Purpose            | Reduces variance and overfitting              | Reduces bias and improves accuracy          |\r\n",
    "| Example Algorithm       | Random Forest                                | AdaBoost, Gradient Boosting Machines (GBM)  |\r\n",
    "| Parallelization         | Can be easily parallelized                   | Sequential nature may limit paralleat a glance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7114d9-e01d-4b70-ac61-08aa9b1f6a5a",
   "metadata": {},
   "source": [
    "### Three main differences between bagging and boosting:\n",
    "\n",
    "1. **Type of Model Used**:\n",
    "   - **Bagging**: Bagging can use any type of model as its base learner. It is flexible in terms of the choice of base model and can accommodate both simple and complex models, such as decision trees, SVMs, or neural networks.\n",
    "   - `Uses Low Bias and High Variance models and try to acheive Low Bias and Low Variance` \n",
    "   - **Boosting**: Boosting typically uses weak learners as its base models. Weak learners are models that perform slightly better than random guessing. Decision stumps (shallow decision trees with a single split) are commonly used as weak learners in boosting algorithms like AdaBoost.\n",
    "   - `Uses High Bias and Low Variance models and try to acheive Low Bias and Low Variance`\n",
    "\n",
    "2. **Sequential vs Parallel**:\n",
    "   - **Bagging**: Bagging trains each base learner independently in parallel. The base learners are trained on different subsets of the training data using bootstrap sampling with replacement. Therefore, bagging can be easily parallelized, allowing for efficient use of computational resources.\n",
    "   - **Boosting**: Boosting trains base learners sequentially. Each base learner is trained based on the performance of the previous learners, focusing more on the instances that were misclassified. Boosting is inherently sequential, as each learner corrects the errors made by its predecessors.\n",
    "\n",
    "3. **Weightage of Base Learner**:\n",
    "   - **Bagging**: In bagging, all base learners have equal weightage when making the final prediction. The final prediction is obtained by averaging (for regression) or voting (for classification) the predictions of all base learners.\n",
    "   - **Boosting**: In boosting, each base learner's weightage depends on its performance. Base learners that perform well (i.e., have lower error rates) are given higher weightage, while those with higher error rates are given lower weightage. The final prediction is a weighted combination of the predictions of all base learners, where the weights are determined based on the performance of each learner.\n",
    "\n",
    "These differences highlight the contrasting approaches of bagging and boosting in leveraging multiple base learners to improve model performance. Bagging focuses on reducing variance and overfitting by averaging predictions from independently trained models, while boosting aims to reduce bias and improve accuracy by iteratively correcting errors made by sequential models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
