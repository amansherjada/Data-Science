{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152a1df1-497e-4d4c-8588-1da571fab0c6",
   "metadata": {},
   "source": [
    "## Batch gradient Descent\n",
    "\n",
    "### Types of Gradient Descent\n",
    "\n",
    "![types](https://editor.analyticsvidhya.com/uploads/58182variations_comparison.png)\n",
    "\n",
    "### Difference Between\n",
    "\n",
    "The main difference between batch gradient descent, stochastic gradient descent (SGD), and mini-batch gradient descent lies in how they process training data and update model parameters during optimization:\r\n",
    "\r\n",
    "1. **Batch Gradient Descent**:\r\n",
    "   - **Processing Data**: In batch gradient descent, the entire training dataset is used to compute the gradient of the loss function with respect to the model parameters in each iteration.\r\n",
    "   - **Update Rule**: Model parameters are updated once per iteration by taking an average gradient over all training examples.\r\n",
    "   - **Advantages**: It provides a precise estimate of the gradient and leads to stable convergence, especially when the loss function is convex.\r\n",
    "   - **Disadvantages**: Computationally expensive for large datasets since it requires processing the entire dataset in each iteration. It may get stuck in local minima for non-convex loss functions.\r\n",
    "\r\n",
    "2. **Stochastic Gradient Descent (SGD)**:\r\n",
    "   - **Processing Data**: In SGD, a single randomly chosen training example (or a small subset) is used to compute the gradient of the loss function in each iteration.\r\n",
    "   - **Update Rule**: Model parameters are updated after computing the gradient for each individual example.\r\n",
    "   - **Advantages**: It converges faster due to frequent updates and is less likely to get stuck in local minima. It is more computationally efficient, especially for large datasets.\r\n",
    "   - **Disadvantages**: It may exhibit high variance in the updates, leading to noisy convergence. The convergence may be less stable compared to batch gradient descent.\r\n",
    "\r\n",
    "3. **Mini-Batch Gradient Descent**:\r\n",
    "   - **Processing Data**: Mini-batch gradient descent strikes a balance between batch gradient descent and SGD. It processes small batches of randomly chosen training examples.\r\n",
    "   - **Update Rule**: Model parameters are updated once per batch by taking an average gradient over the examples in the batch.\r\n",
    "   - **Advantages**: It combines the stability of batch gradient descent with the efficiency of SGD. It can make efficient use of hardware optimizations like parallelism when computing gradients.\r\n",
    "   - **Disadvantages**: It introduces a hyperparameter (batch size) that needs to be tuned. The choice of batch size can affect convergence speed and efficiency.\r\n",
    "\r\n",
    "In summary, the main differences lie in the amount of data used to compute gradients (entire dataset, single example, or small batches), the frequency of parameter updates, and the computational efficiency. Each variant has its trade-offs in terms of convergence speed, stability, and computational resources. The choice depends on the specific problem, dataset size, and computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233a23ca-dbbc-437d-86c5-4cc515030f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119dccf0-ae00-4596-9247-8d8c89f93a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b68db84-eabe-4b4b-bcd1-8589b37276a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285cb812-e9c8-4b91-beae-e1f039e6c28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4915f3ab-9aef-40c8-832c-1ea3ae3644b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5887fc0-8c0f-426a-955b-55ae9d4060d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31b50ce-4770-4fd1-82c3-e50ee8d49e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -9.15865318, -205.45432163,  516.69374454,  340.61999905,\n",
       "       -895.5520019 ,  561.22067904,  153.89310954,  126.73139688,\n",
       "        861.12700152,   52.42112238])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 10 columns so we will get 10 coefficient (β1, β2,.....β10)\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae6e09f-af2f-45ab-9218-1be88fbc8524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.88331005254167"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercept (β0)\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f2d1e5-dfc9-44c2-9143-8fbdfb7fc590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399338661568968"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811f8851-2113-4e76-a074-01faea7fc614",
   "metadata": {},
   "source": [
    "### Making our own Gradient Descent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9bc756-40bc-41e6-872c-e83a14618ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This show how many columns are there in our dataset\n",
    "# 10 cols = 10 coef\n",
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c88eeb-a53d-4172-8cc5-fcb59fc25ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class amanGD:\n",
    "    def __init__(self, learning_rate = 0.1, epochs = 100):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # initialize coef_ and intercept\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            #Update all coef and intercept\n",
    "            # Updating intercept\n",
    "            # 1. calculating y_hat\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
    "            # 2. Calculating derivative of a loss function with respect to the intercept parameter ∂MSE(L) / ∂intercept(β0) = (2/N)*Σ(i=1 to N) (y_i - ŷ_i)\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            # 3. updating the intercept parameter using a learning rate\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "            # Updating All coefficients\n",
    "            # 1. derivative of a loss function with respect to the coefficients ∂MSE(L) / ∂coefficients(βm) = (2/N)*Σ(i=1 to N) (y_i - ŷ_i)(x_im) here, m= columns\n",
    "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
    "            # 2. updating the coefficients (or weights) using gradient descent optimization\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        print(self.intercept_,self.coef_)\n",
    "    def predict(self, X_test):\n",
    "        # y = (βm x 1) + β0\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb4079-741c-4592-b873-f74ccfb0b26a",
   "metadata": {},
   "source": [
    "For Updating All coefficients refer this video\n",
    "\n",
    "[![Alt text](https://img.youtube.com/vi/Jyo53pAyVAM/0.jpg)](https://www.youtube.com/watch?v=Jyo53pAyVAM?t=3284)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21b04689-10c6-43c8-a20e-21f10e8abe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = amanGD(epochs=1000, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c41a170-5f83-4494-b4fa-11e2f0aabba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.01351687661833 [  14.38990585 -173.7235727   491.54898524  323.91524824  -39.32648042\n",
      " -116.01061213 -194.04077415  103.38135565  451.63448787   97.57218278]\n"
     ]
    }
   ],
   "source": [
    "gd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8c63d2-53a2-48b3-a42f-8c000e6da414",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "290627b8-b6f8-44c6-bf4b-0c1d889545b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4534503034722803"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d1fa1-bbbc-4843-a3c6-61660cb658e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
