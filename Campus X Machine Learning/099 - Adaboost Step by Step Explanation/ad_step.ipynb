{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e032c88-336b-4e91-a62d-e45ca16b49c3",
   "metadata": {},
   "source": [
    "Explaination = https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/\n",
    "\n",
    "id = koxobah285@storesr.com\n",
    "pass = m*eyB6YCyujSkL3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82b909-b27a-4192-ae25-bd35b8ccb339",
   "metadata": {},
   "source": [
    "Here's a step-by-step explanation of the AdaBoost algorithm:\r\n",
    "\r\n",
    "1. **Initialize Weights**: Assign equal weights to each training example in the dataset. Initially, each sample has the same importance.\r\n",
    "\r\n",
    "2. **For each iteration \\( t \\)**:\r\n",
    "   - **a. Train Weak Learner**: Train a weak learner (often a decision stump, which is a decision tree with a single split) on the training data. The weak learner focuses on minimizing the weighted error rate.\r\n",
    "   - **b. Compute Error**: Compute the weighted error rate of the weak learner. This is the sum of the weights of the misclassified samples divided by the total weight of all samples.\r\n",
    "   - **c. Compute Learner Weight**: Calculate the weight \\( \\alpha_t \\) of the weak learner based on its error rate. This weight determines the contribution of the weak learner to the final prediction.\r\n",
    "   - **d. Update Sample Weights**: Update the weights of the training samples. Increase the weights of misclassified samples and decrease the weights of correctly classified samples. This emphasizes the importance of difficult-to-classify samples in subsequent iterations.\r\n",
    "   \r\n",
    "3. **Combine Weak Learners**:\r\n",
    "   - Combine the weak learners into a strong classifier by weighting their predictions based on \\( \\alpha_t \\). Weak learners with lower error rates typically have higher weights.\r\n",
    "\r\n",
    "4. **Final Prediction**:\r\n",
    "   - To make predictions for new data points, combine the predictions of all weak learners using their weighted votes. The final prediction is determined by summing up the weighted predictions and applying a sign function.\r\n",
    "\r\n",
    "5. **Repeat or Terminate**:\r\n",
    "   - Repeat steps 2-4 for a predefined number of iterations (or until a perfect fit is achieved) or until a stopping criterion is met (such as reaching a desired accuracy).\r\n",
    "\r\n",
    "6. **Output**:\r\n",
    "   - The final model consists of the weighted combination of weak learners, and it can be used to make predictions on unseen data.\r\n",
    "\r\n",
    "In summary, AdaBoost iteratively trains weak learners, giving more emphasis to misclassified samples in each iteration. It combines these weak learners into a strong classifier by weighting their predictions based on their accuracy. This iterative process leads to the creation of a robust ensemble model capable of handling complex datasets and achieving high accuracy in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aedfbc-3241-4802-af1a-3ac75326b1af",
   "metadata": {},
   "source": [
    "Reference Video for train to update pdf = https://youtu.be/RT0t9a3Xnfw?si=mZ95d7Fy0_6lxhJi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
