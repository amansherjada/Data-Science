{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7f6a3e-0589-440a-a5ed-0475069a4b89",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192dd764-e32a-4547-bbbe-60426c0e2987",
   "metadata": {},
   "source": [
    "### After applying Ridge Regression using Gradient Descent, the coefficients get affected in the following way:\r\n",
    "\r\n",
    "1. **Regularization effect:**\r\n",
    "   - Ridge regression adds a regularization term (L2 penalty) to the cost function. This penalty term is proportional to the square of the magnitude of the coefficients.\r\n",
    "   - During the training process, as the algorithm iterates to minimize the cost function, it considers not only the error between the predicted and actual values (the mean squared error) but also penalizes large coefficient values.\r\n",
    "   - As a result, the coefficients are pushed towards smaller values compared to ordinary least squares regression, where there's no penalty for large coefficients.\r\n",
    "\r\n",
    "2. **Shrinkage of coefficients:**\r\n",
    "   - The effect of regularization is to shrink the coefficients towards zero. Features that are less relevant to the target variable tend to have their corresponding coefficients reduced more significantly.\r\n",
    "   - The amount of shrinkage applied to each coefficient depends on the regularization parameter (`alpha`). A higher `alpha` value results in stronger regularization and greater shrinkage of coefficients.\r\n",
    "\r\n",
    "3. **Bias-variance tradeoff:**\r\n",
    "   - Ridge regression helps in reducing overfitting by shrinking the coefficients, which effectively reduces the variance of the model.\r\n",
    "   - However, this comes at the cost of introducing some bias into the model. The bias is introduced because the model may not perfectly fit the training data, as it is penalized for having large coefficients.\r\n",
    "\r\n",
    "In summary, applying Ridge Regression with Gradient Descent affects the coefficients by shrinking them towards zero, which helps in reducing overfitting and improving the generalization ability of the model. The degree of shrinkage depends on the regularization parameter (`alpha`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ad6c6-1f41-4ffd-98b5-e1f5c467e793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
