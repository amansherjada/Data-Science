{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a27d9d6b-073c-43e1-8fda-33bcb3e3bf8b",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "![](https://velog.velcdn.com/images%2Fjiselectric%2Fpost%2F49803ffd-d915-403f-8c78-9fe5ee26ad1d%2F%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-01-14%20%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB%201.02.01.png)\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "In Voting Ensemble, there are several assumptions and considerations to keep in mind to ensure the effectiveness and reliability of the ensemble approach:\r\n",
    "\r\n",
    "1. **Independence of Base Models**: It is assumed that the individual base models in the ensemble are trained independently of each other. If the models are trained on similar data or are highly correlated, the ensemble may not achieve significant performance improvements.\r\n",
    "\r\n",
    "2. **Base Model Diversity**: The effectiveness of the ensemble often depends on the diversity among the base models. Diversity ensures that different models capture different aspects of the data and make errors in different regions of the feature space. Ensuring diversity among the base models can lead to more accurate and robust ensemble predictions.\r\n",
    "\r\n",
    "3. **Comparable Performance**: Ideally, the individual base models in the ensemble should have comparable performance levels. If one model significantly outperforms the others, its predictions may dominate the ensemble, potentially reducing the ensemble's effectiveness. Ensuring similar performance levels among base models helps maintain a balanced contribution to the final prediction.\r\n",
    "\r\n",
    "4. **Appropriate Aggregation Method**: The aggregation method used in the ensemble (e.g., hard voting, soft voting) should be appropriate for the problem at hand and the characteristics of the individual base models. For example, soft voting may be more suitable when base models output probabilities or confidence scores, while hard voting may be preferred when dealing with discrete class labels.\r\n",
    "\r\n",
    "5. **Large and Diverse Ensemble**: In general, larger and more diverse ensembles tend to perform better than smaller ones. However, there is a trade-off between ensemble size and computational complexity. It is essential to strike a balance between the number of base models in the ensemble and computational resources available.\r\n",
    "\r\n",
    "6. **Homogeneous vs. Heterogeneous Ensembles**: Depending on the problem and dataset, either homogeneous (base models of the same type) or heterogeneous (base models of different types) ensembles may be appropriate. The choice between homogeneous and heterogeneous ensembles should be guided by the desired level of diversity and the characteristics of the dataset.\r\n",
    "\r\n",
    "7. **Data Quality and Consistency**: The quality and consistency of the training data used to train individual base models can significantly impact the performance of the ensemble. Ensuring high-quality, representative training data is essential for achieving reliable ensemble predictions.\r\n",
    "\r\n",
    "By considering these assumptions and considerations, practitioners can design and implement Voting Ensembles effectively, leading to improved predictive performance and robustness compared to individual models.\n",
    "\n",
    "#### Core Idea\n",
    "\n",
    "The core idea behind a Voting Ensemble, also known as a Majority Voting Ensemble, is to combine predictions from multiple individual models and make a final prediction based on the most common prediction among the individual models. \r\n",
    "\r\n",
    "Here's how a Voting Ensemble typically works:\r\n",
    "\r\n",
    "1. **Training Multiple Models**: Several individual models are trained independently on the same dataset using different algorithms or variations of the same algorithm. These models can be of various types, such as decision trees, logistic regression, support vector machines, etc.\r\n",
    "\r\n",
    "2. **Making Predictions**: Once the individual models are trained, each model independently predicts the target variable (for classification tasks) or outputs a continuous value (for regression tasks) for each instance in the dataset.\r\n",
    "\r\n",
    "3. **Aggregating Predictions**: In the case of classification tasks, the predictions from individual models are combined by taking a majority vote. The class label that receives the most votes among the predictions is considered the final prediction of the ensemble. For regression tasks, the predictions are typically averaged to produce the final prediction.\r\n",
    "\r\n",
    "4. **Final Prediction**: The final prediction of the Voting Ensemble is the aggregated prediction obtained from combining the predictions of individual models.\r\n",
    "\r\n",
    "The Voting Ensemble takes advantage of the \"wisdom of the crowd\" principle, where the collective opinion of multiple models tends to be more accurate than that of any single model. By combining predictions from diverse models, the Voting Ensemble can often achieve better predictive performance, improved generalization, and increased robustness compared to individual models.\r\n",
    "\r\n",
    "There are two main types of Voting Ensembles:\r\n",
    "\r\n",
    "- **Hard Voting**: In hard voting, each individual model gives a discrete prediction (class label) for each instance, and the most common class label among the predictions becomes the final prediction of the ensemble.\r\n",
    "\r\n",
    "- **Soft Voting**: In soft voting, each individual model outputs probabilities or confidence scores for each class, and the final prediction is made by averaging these probabilities across all models and selecting the class with the highest average probability.\r\n",
    "\r\n",
    "Voting Ensembles are easy to implement, versatile, and can be used with a wide range of machine learning algorithms. They are particularly useful when dealing with diverse datasets or when combining models of varying strengths to achieve better overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae19ce-6c96-47a6-b2da-2a7d912c2971",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's illustrate how a Voting Ensemble works with a simple example of a classification task involving predicting whether a student will pass or fail an exam based on two features: hours studied and previous exam scores. We'll create three individual models: a decision tree, a logistic regression, and a k-nearest neighbors (KNN) classifier. Then, we'll use a majority voting ensemble to combine their predictions.e:\r\n",
    "\r\n",
    "#### Individual Models:\r\n",
    "1. **Decision Tree (Model 1)**:\r\n",
    "   - Predicts pass (1) if the student studied more than 5 hours and had a previous exam score higher than 70; otherwise, predicts fail (0).\r\n",
    "\r\n",
    "2. **Logistic Regression (Model 2)**:\r\n",
    "   - Predicts pass (1) if the logistic regression probability score is greater than 0.6; otherwise, predicts fail (0).\r\n",
    "\r\n",
    "3. **K-Nearest Neighbors (KNN) (Model 3)**:\r\n",
    "   - Predicts pass (1) if the majority of the 3 nearest neighbors had passed; otherwise, predicts fail (0).\r\n",
    "\r\n",
    "#### Voting Ensemble:\r\n",
    "- **Hard Voting**: Each model's prediction is considered a \"vote.\" The final prediction is determined by the majority vote among the individual models.\r\n",
    "\r\n",
    "#### Scenario:\r\n",
    "- **Student A**: Studied for 6 hours and scored 80 in the previous exam.\r\n",
    "- **Student B**: Studied for 4 hours and scored 60 in the previous exam.\r\n",
    "\r\n",
    "#### Predictions:\r\n",
    "1. **Model 1 (Decision Tree)**:\r\n",
    "   - Student A: Predicts pass (1).\r\n",
    "   - Student B: Predicts fail (0).\r\n",
    "   \r\n",
    "2. **Model 2 (Logistic Regression)**:\r\n",
    "   - Student A: Predicts pass (1).\r\n",
    "   - Student B: Predicts fail (0).\r\n",
    "   \r\n",
    "3. **Model 3 (KNN)**:\r\n",
    "   - Student A: Predicts pass (1).\r\n",
    "   - Student B: Predicts fail (0).\r\n",
    "\r\n",
    "#### Voting Ensemble (Hard Voting):\r\n",
    "- **Final Prediction**:\r\n",
    "   - For Student A: Two out of three models predict pass (1).\r\n",
    "   - For Student B: Two out of three models predict fail (0).\r\n",
    "- **Conclusion**:\r\n",
    "   - Student A is predicted to pass the exam.\r\n",
    "   - Student B is predicted to fail the exam.\r\n",
    "\r\n",
    "### Summary:\r\n",
    "In this example, the Voting Ensemble combines the predictions of three individual models using hard voting. By taking the majority vote, it aggregates the predictions to make the final prediction. The Voting Ensemble improves the overall prediction accuracy and robustness by leveraging the collective wisdom of multiple models, leading to more reliable predictions compared to any individual model alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
