{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a848dca-7a34-4563-8e0a-ba9e0db9aeaa",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent\n",
    "\n",
    "Mini-batch gradient descent is a variation of the gradient descent optimization algorithm used in machine learning for training models. It combines the advantages of both batch gradient descent (BGD) and stochastic gradient descent (SGD). Here's a simple explanation:\r\n",
    "\r\n",
    "1. **Batch Gradient Descent (BGD)**: In BGD, you compute the gradient of the cost function with respect to the parameters using the entire dataset. Then, you update the parameters once based on this average gradient.\r\n",
    "\r\n",
    "2. **Stochastic Gradient Descent (SGD)**: In SGD, you compute the gradient of the cost function with respect to the parameters using only one data point (or a small subset, called a mini-batch) randomly chosen from the dataset. Then, you update the parameters based on this individual gradient.\r\n",
    "\r\n",
    "3. **Mini-Batch Gradient Descent**: Mini-batch gradient descent strikes a balance between BGD and SGD. Instead of using the entire dataset (BGD) or just one data point (SGD), mini-batch gradient descent computes the gradient using a small random subset (mini-batch) of the dataset. Then, it updates the parameters based on this mini-batch gradient. This process is repeated for multiple mini-batches until convergence.\r\n",
    "\r\n",
    "In summary, mini-batch gradient descent combines the efficiency of SGD, which updates parameters more frequently and requires less memory, with the stability of BGD, which provides a more accurate estimate of the gradient. This makes it a popular choice for training models, especially in scenarios where the dataset is large and computational resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17b2ca3-9a0f-4eaa-91f1-898c984ebe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45133e38-61fa-4449-8079-8852dce9a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151bdb11-3b54-4402-b612-d428dee314ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9736e8f5-d1a8-4c74-b583-5020dfac6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fa975b-6b62-49b4-8a25-ffb0132e4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e828ed0b-21ac-4d7d-8c4b-e7be339b7145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9022cd33-2d91-4339-8019-5720b0243cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b1adbb-a347-4189-9767-0827864a78b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4399338661568968"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0bc7cd1-e022-4f08-93d8-29bd0c2ee295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -9.15865318, -205.45432163,  516.69374454,  340.61999905,\n",
       "       -895.5520019 ,  561.22067904,  153.89310954,  126.73139688,\n",
       "        861.12700152,   52.42112238])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e801637a-5ffd-43a8-af6d-2ce3a274cc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.88331005254167"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc0a21a7-fa10-40c4-8658-0e6d657ce36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad3193-7712-4166-9deb-0438adb3cfdc",
   "metadata": {},
   "source": [
    "### Building our own Mini Batch GD class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e817c9c0-a36d-4703-a217-06304fdc7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np  # Importing numpy library for numerical operations\n",
    "\n",
    "class MBGDRegressor:\n",
    "    \n",
    "    def __init__(self, batch_size, learning_rate=0.01, epochs=100):\n",
    "        \"\"\"\n",
    "        Constructor method to initialize the parameters of the model.\n",
    "        \n",
    "        Args:\n",
    "        - batch_size: Size of the mini-batch for mini-batch gradient descent\n",
    "        - learning_rate: Learning rate for the gradient descent updates\n",
    "        - epochs: Number of epochs (iterations over the entire dataset) for training\n",
    "        \n",
    "        \"\"\"\n",
    "        # Initialize model parameters\n",
    "        self.coef_ = None  # Coefficients of the linear regression model\n",
    "        self.intercept_ = None  # Intercept term of the linear regression model\n",
    "        self.lr = learning_rate  # Learning rate for gradient descent\n",
    "        self.epochs = epochs  # Number of training epochs\n",
    "        self.batch_size = batch_size  # Size of mini-batch\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Method to train the linear regression model using mini-batch gradient descent.\n",
    "        \n",
    "        Args:\n",
    "        - X_train: Training features (input data)\n",
    "        - y_train: Training labels (output data)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Initialize coefficients\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        # Iterate over epochs\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # Iterate over mini-batches\n",
    "            for j in range(int(X_train.shape[0] / self.batch_size)):\n",
    "                \n",
    "                # Randomly sample indices for mini-batch\n",
    "                idx = random.sample(range(X_train.shape[0]), self.batch_size)\n",
    "                \n",
    "                # Compute predictions\n",
    "                y_hat = np.dot(X_train[idx], self.coef_) + self.intercept_\n",
    "                \n",
    "                # Compute derivatives of cost function wrt intercept and coefficients\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat), X_train[idx])\n",
    "                \n",
    "                # Update intercept and coefficients using gradient descent\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        # Print the final values of intercept and coefficients after training\n",
    "        print(self.intercept_, self.coef_)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Method to predict outputs for new input data.\n",
    "        \n",
    "        Args:\n",
    "        - X_test: Test features (input data)\n",
    "        \n",
    "        Returns:\n",
    "        - Predicted outputs\n",
    "        \n",
    "        \"\"\"\n",
    "        # Compute predictions using learned coefficients\n",
    "        return np.dot(X_test, self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6fabb3-37b3-4f00-a817-ec5970fdf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr = MBGDRegressor(batch_size=int(X_train.shape[0]/50),learning_rate=0.01,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f571c45-7d24-486f-a688-6ff2491d203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.37615116301237 [  24.70118157 -137.26292063  456.14461743  295.31504016  -25.442493\n",
      "  -95.3148191  -193.36400333  118.95666971  410.82375837  118.26988043]\n"
     ]
    }
   ],
   "source": [
    "mbr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e95f0115-d6c2-41e8-b54c-4bc9ea9c9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5355439f-3049-48aa-9aec-266751d195f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45355345420730653"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629a204-55ca-49f3-b62d-b93ca715423a",
   "metadata": {},
   "source": [
    "![](https://github.com/campusx-official/100-days-of-machine-learning/blob/main/day52-types-of-gradient-descent/mini_batch_contour_plot.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b377a-6f97-4538-b9c0-9426d072974b",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aafa6fa-4b5a-4758-8a55-3565dd645e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cf0cccf-ff84-4e2d-95aa-2cf6a871fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(learning_rate='constant',eta0=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffbb2e-6fea-4d32-8b11-eb95ce5da152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "534e4f8b-a43f-4f89-837a-f097f2ada8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  There is no way to put batch size in the SGDRegressor, so we are using this to apply batch size to the mini batch in the SGDRegressor class.\n",
    "batch_size = 35\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    idx = random.sample(range(X_train.shape[0]),batch_size)\n",
    "    sgd.partial_fit(X_train[idx],y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1646d-40db-4408-ad20-2c569dd850fc",
   "metadata": {},
   "source": [
    "Let's break down what each part does:\r\n",
    "\r\n",
    "1. `batch_size = 35`: This line sets the batch size to 35. In SGD, instead of using the entire dataset at once (as in batch gradient descent), we use only a subset of the dataset for each update step. This subset is called a mini-batch, and its size is specified by the `batch_size`.\r\n",
    "\r\n",
    "2. `for i in range(100):`: This loop runs for 100 iterations, or epochs. During each epoch, the model is updated multiple times using mini-batches of data.\r\n",
    "\r\n",
    "3. `idx = random.sample(range(X_train.shape[0]), batch_size)`: This line randomly selects `batch_size` indices from the range of indices corresponding to the training data (`X_train`). This effectively creates a random mini-batch of training data.\r\n",
    "\r\n",
    "4. `sgd.partial_fit(X_train[idx], y_train[idx])`: This line fits (or trains) the model using the current mini-batch. The `partial_fit` method is commonly used in online learning scenarios, where the model is updated incrementally as new data becomes available. Here, `X_train[idx]` represents the features (input data) of the mini-batch, and `y_train[idx]` represents the corresponding target labels (output data).\r\n",
    "\r\n",
    "Overall, this code performs 100 iterations of SGD training, where in each iteration, a random mini-batch of 35 data points is selected, and the model is updated using these data points. This process allows the model to gradually improve its performance over multiple epochs while efficiently utilizing the available training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de01ed75-3484-4d3a-9950-112b012eae62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  42.50833574,  -59.73770479,  348.25917255,  240.13042681,\n",
       "         26.35450936,  -25.87807718, -158.42251649,  117.41868933,\n",
       "        316.19495841,  144.40076166])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33a73ed3-4e59-45c2-b9b9-7e74cbd22b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([146.90314164])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "478878ca-eb88-43c9-81e3-71cfce8af659",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dfca700-5ee7-4e9c-a783-b2125a6d23ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42580121632640955"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef7daa-3da4-453b-bceb-df622bfc96c0",
   "metadata": {},
   "source": [
    "### When to use Mini Batch gradient descent?\r\n",
    "\r\n",
    "1. **Dealing with Large Datasets**: If your dataset is too large to fit into memory, mini-batch gradient descent allows you to efficiently process it by dividing it into smaller chunks (mini-batches).\r\n",
    "\r\n",
    "2. **Balancing Speed and Stability**: Mini-batch gradient descent strikes a balance between the speed of stochastic gradient descent (SGD) and the stability of batch gradient descent (BGD). It updates the model parameters more frequently than BGD but with less noise compared to SGD.\r\n",
    "\r\n",
    "3. **Improving Generalization**: Mini-batch gradient descent can lead to better generalization by averaging gradients over mini-batches, which helps the optimization process converge to a more robust solution.\r\n",
    "\r\n",
    "4. **Efficient Computation**: It's suitable for modern hardware architectures like GPUs, allowing for parallelization and vectorized operations, making computation more efficient.\r\n",
    "\r\n",
    "5. **Control over Stochasticity**: You can control the amount of stochasticity in the optimization process by adjusting the batch size, balancing computational efficiency with the stability of updates.\r\n",
    "\r\n",
    "6. **Online Learning**: Mini-batch gradient descent is great for scenarios where the model needs to be continuously updated with new data (online learning), as it allows for incremental updates without retraining on the entire dataset.\r\n",
    "\r\n",
    "In essence, mini-batch gradient descent is a versatile choice for optimizing machine learning models, especially when dealing with large datasets and efficiency is a concern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
