{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb06df70-8930-42c1-9530-513d450baaac",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a Feature Extraction\n",
    "PCA is a Unsupervised Approach\n",
    "Reduce COD\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical method used for dimensionality reduction in data analysis and visualization. It works by transforming high-dimensional data into a new coordinate system, where the axes (called principal components) are orthogonal to each other and ordered by the amount of variance they explain in the original data.\n",
    "\n",
    "How PCA works:\n",
    "\n",
    "1. **Standardization**: PCA typically starts with standardizing the data to have a mean of 0 and a standard deviation of 1 across each feature. This step ensures that each feature contributes equally to the analysis.\n",
    "\n",
    "2. **Covariance Matrix Calculation**: PCA then calculates the covariance matrix of the standardized data. The covariance matrix represents the relationships between all pairs of features in the dataset.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: The next step involves decomposing the covariance matrix into its eigenvectors and eigenvalues. Eigenvectors represent the directions (principal components) of maximum variance in the data, and eigenvalues represent the magnitude of variance along each eigenvector.\n",
    "\n",
    "4. **Selection of Principal Components**: The eigenvectors are ranked by their corresponding eigenvalues, with the highest eigenvalue indicating the direction of maximum variance in the data. These eigenvectors are the principal components. Typically, only the top k eigenvectors (where k is the desired number of dimensions for the reduced dataset) are retained.\n",
    "\n",
    "5. **Projection**: Finally, the original data is projected onto the new coordinate system defined by the selected principal components. Each data point is represented as a linear combination of the principal components.\n",
    "\n",
    "Here's a simple example:\n",
    "\n",
    "Let's say you have a dataset with two features: height and weight of individuals. Each data point represents a person.\n",
    "\n",
    "1. Standardize the data so that both height and weight have a mean of 0 and a standard deviation of 1.\n",
    "2. Calculate the covariance matrix of the standardized data.\n",
    "3. Decompose the covariance matrix into eigenvectors and eigenvalues.\n",
    "4. Select the top eigenvector as the first principal component (direction of maximum variance).\n",
    "5. Project the original data onto the first principal component axis.\n",
    "\n",
    "After PCA, you would have transformed the original dataset into a new coordinate system where the principal components capture the most significant variation in the data. This reduced representation can help with visualization, analysis, and sometimes even improve the performance of machine learning algorithms by reducing noise and redundancy in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af60e5-1cdd-45b2-8927-7d399c1fe577",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/1*37a_i1t1tDxDYT3ZI6Yn8w.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
