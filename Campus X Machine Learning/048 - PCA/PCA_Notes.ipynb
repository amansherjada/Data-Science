{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f06624-394f-41bb-a74b-f4d70703ba7e",
   "metadata": {},
   "source": [
    "**Variance**:\n",
    "- Variance measures the dispersion of a single random variable from its mean or expected value. \n",
    "- It quantifies how much a random variable deviates from its mean.\n",
    "- For a random variable X, the variance is calculated as `Var(X) = E[(X - µ)^2]`, where E denotes the expectation (or mean) and µ is the mean of X.\n",
    "- In simpler terms, variance tells us how much the values of a single variable tend to deviate from the mean.\n",
    "\n",
    "**Covariance**:\n",
    "- Covariance measures the degree to which two random variables change together.\n",
    "- It indicates the direction of the linear relationship between two variables (whether they tend to increase or decrease together) and the strength of that relationship.\n",
    "- For two random variables X and Y, the covariance is calculated as `Cov(X, Y) = E[(X - µ_X)(Y - µ_Y)]`, where µ_X and µ_Y are the means of X and Y, respectively.\n",
    "- Covariance can be positive (indicating that as one variable increases, the other tends to increase), negative (indicating that as one variable increases, the other tends to decrease), or zero (indicating no linear relationship).\n",
    "- However, covariance doesn't give a standardized measure, making it difficult to interpret the strength of the relationship between variables, as it is dependent on the scales of the variables.\n",
    "\n",
    "**Covariance Matrix**:\n",
    "- A covariance matrix is a square matrix that summarizes the variances and covariances between multiple variables.\n",
    "- If you have n variables, the covariance matrix will be an n x n matrix.\n",
    "- The diagonal elements of the covariance matrix represent the variances of the individual variables, and the off-diagonal elements represent the covariances between pairs of variables.\n",
    "- A covariance matrix is symmetric, with each element `Cov(X_i, X_j) equal to Cov(X_j, X_i)`, ensuring that the covariance between variables i and j is the same as the covariance between j and i.\n",
    "- Covariance matrices are essential in multivariate analysis, including techniques like Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), where understanding the relationships between multiple variables is crucial for analysis.\n",
    "\n",
    "simplified version:\n",
    "\n",
    "**Variance**:\n",
    "- Variance measures how much individual data points differ from the average.\n",
    "- It tells us the spread or dispersion of a single set of data.\n",
    "- For example, if you're looking at test scores, variance indicates how much each score differs from the average score.\n",
    "\n",
    "**Covariance**:\n",
    "- Covariance measures how two sets of data change together.\n",
    "- It shows whether two sets of data tend to increase or decrease at the same time.\n",
    "- For example, if you're looking at both temperature and ice cream sales, covariance tells you if hotter days tend to have more ice cream sales.\n",
    "\n",
    "**Covariance Matrix**:\n",
    "\n",
    "<div style=\"text-align:center;\">\r\n",
    "    <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230822231834/image-(2).png\" alt=\"Image\" width=\"400\" height=\"300\">\r\n",
    "</div\r\n",
    "\n",
    "\n",
    "- A covariance matrix summarizes how all the different sets of data in a dataset are related.\n",
    "- It's like a big table that shows how every pair of data sets changes together.\n",
    "- Covariance matrices are used in techniques like Principal Component Analysis (PCA) to understand relationships between different features in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c403d0-fdcd-487e-bb9d-d9e39507e314",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\r\n",
    "    <img src=\"https://cdn.educba.com/academy/wp-content/uploads/2019/01/Variance-vs-Covariance-info.jpg.webp\" alt=\"Variance vs Covariance\" width=\"400\" height=\"300\">\r\n",
    "</di>\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa8b67-2985-44cc-af8e-60c42e8f6646",
   "metadata": {},
   "source": [
    "\r\n",
    "**Eigenvalues**:\r\n",
    "- Eigenvalues represent how much a matrix transformation stretches or shrinks vectors in space.\r\n",
    "- They are single values associated with a matrix.\r\n",
    "- Eigenvalues tell us the factor by which the corresponding eigenvector is scaled during the transformation.\r\n",
    "- They are solutions to the characteristic equation det(A - λI) = 0, where λ is the eigenvalue, A is the matrix, and I is the identity matrix.\r\n",
    "\r\n",
    "**Eigenvectors**:\r\n",
    "- Eigenvectors are vectors that remain in the same direction after a linear transformation.\r\n",
    "- They are non-zero vectors associated with eigenvalues.\r\n",
    "- If a matrix A is multiplied by one of its eigenvectors v, the resulting vector will be parallel to the original v (though it may be scaled by its eigenvalue).\r\n",
    "- Eigenvectors are solutions to the equation Av = λv, where v is the eigenvector and λ is the corresponding eigenvalue.\r\n",
    "\r\n",
    "In summary, eigenvalues tell us how much a matrix transformation stretches or shrinks vectors, while eigenvectors are the vectors that remain in the same direction (up to scaling) after the transformation. They are crucial concepts in linear algebra with applications across various fields. and engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6629d1-343b-4d45-9666-042e610a9084",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) uses eigenvectors and eigenvalues because it aims to find the directions of maximum variance in the data space. Eigenvectors and eigenvalues provide a mathematical way to determine these directions and quantify the amount of variance along each direction.\r\n",
    "\r\n",
    "Here's why PCA uses eigenvectors and eigenvalues:\r\n",
    "\r\n",
    "1. **Capture Variance**: PCA seeks to transform the original data into a new set of variables (principal components) that capture the maximum amount of variance in the data. Eigenvectors represent these principal components, and eigenvalues indicate the amount of variance explained by each principal component.\r\n",
    "\r\n",
    "2. **Orthogonality**: Eigenvectors are orthogonal to each other, meaning they are perpendicular in the data space. This ensures that each principal component captures a unique direction of variance in the data, without redundancy.\r\n",
    "\r\n",
    "3. **Dimensionality Reduction**: PCA sorts the eigenvectors based on their corresponding eigenvalues in descending order. This allows PCA to select the top k eigenvectors (where k is the desired number of dimensions for the reduced dataset) that capture the most variance. By doing so, PCA reduces the dimensionality of the data while retaining as much information as possible.\r\n",
    "\r\n",
    "4. **Linear Transformation**: PCA transforms the original data by projecting it onto the subspace defined by the selected principal components. This transformation is a linear operation, and eigenvectors provide the basis for this transformation.\r\n",
    "\r\n",
    "In summary, PCA uses eigenvectors and eigenvalues to identify the principal components that capture the most variance in the data and to perform the linear transformation necessary for dimensionality reduction. They provide a mathematical framework for understanding and implementing PCA effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901d8eb-be92-4645-a687-f631279bdd3c",
   "metadata": {},
   "source": [
    "### Step to solve PCA (Play this video)\n",
    "\n",
    "<div style=\"text-align:center;\">\r\n",
    "    <a href=\"http://www.youtube.com/watch?v=tXXnxjj2wM4&t=2286&end=2618\">\r\n",
    "        <img src=\"http://img.youtube.com/vi/tXXnxjj2wM4/0.jpg\" alt=\"Step by Step\" width=\"600\">\r\n",
    "    </a>\r\n",
    "</div>\r\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
