{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cd2d23-4e60-459e-aa1e-8962b6b3298c",
   "metadata": {},
   "source": [
    "### Naive Bayes intuition\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem with an assumption of independence among features. Here's a step-by-step intuition for understanding Naive Bayes:\n",
    "\n",
    "1. **Understand Bayes' Theorem**: At its core, Naive Bayes relies on Bayes' Theorem, which is a fundamental concept in probability theory. Bayes' Theorem calculates the probability of a hypothesis (H) given the evidence (E), expressed as P(H|E), using the probability of the evidence given the hypothesis (P(E|H)), the probability of the hypothesis (P(H)), and the probability of the evidence (P(E)).\n",
    "\n",
    "    Mathematically, it's represented as:\n",
    "    \n",
    "    P(H|E) = (P(E|H) * P(H)) / P(E)\n",
    "\n",
    "2. **Assumption of Independence**: Naive Bayes assumes that the features are conditionally independent given the class label. This means that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "3. **Training the Model**: During the training phase, Naive Bayes calculates the probability of each class and the probability of each feature given each class. It computes these probabilities from the training data.\n",
    "\n",
    "4. **Classification**:\n",
    "   - **Input**: Given a new instance with features X, we want to predict the class Y.\n",
    "   - **Calculate Prior Probability**: Calculate the prior probability of each class (P(Y)).\n",
    "   - **Calculate Likelihood**: Calculate the likelihood of the features given each class (P(X|Y)). This involves assuming that the features are independent given the class.\n",
    "   - **Posterior Probability**: Use Bayes' theorem to calculate the posterior probability of each class given the features (P(Y|X)).\n",
    "   - **Decision Rule**: Select the class with the highest posterior probability as the predicted class.\n",
    "\n",
    "5. **Example**:\n",
    "   Let's consider a simple example of classifying whether an email is spam or not based on two features: \n",
    "   - X1: Contains the word \"free\" (1 if present, 0 if not)\n",
    "   - X2: Contains the word \"buy\" (1 if present, 0 if not)\n",
    "   And the class label:\n",
    "   - Y: Spam (1) or Not Spam (0)\n",
    "   \n",
    "   We have a training dataset with the following instances:\n",
    "   \n",
    "   | Example | Contains \"free\" (X1) | Contains \"buy\" (X2) | Class (Y) |\n",
    "   |---------|----------------------|---------------------|-----------|\n",
    "   | 1       | 1                    | 0                   | 1         |\n",
    "   | 2       | 0                    | 1                   | 1         |\n",
    "   | 3       | 1                    | 1                   | 1         |\n",
    "   | 4       | 0                    | 1                   | 0         |\n",
    "   | 5       | 0                    | 0                   | 0         |\n",
    "\n",
    "   Now, let's say we have a new email with the features (1, 1). We want to classify it as spam or not spam using Naive Bayes.\n",
    "   \n",
    "   - Prior Probability: P(Y=1) = 3/5, P(Y=0) = 2/5\n",
    "   - Likelihood:\n",
    "        - P(X1=1|Y=1) = 2/3 ≈ 0.67\n",
    "        - P(X2=1|Y=1) = 2/3 ≈ 0.67\n",
    "        - P(X1=1|Y=0) = 0/2 = 0\n",
    "        - P(X2=1|Y=0) = 1/2 = 0.5\n",
    "   \n",
    "   - Posterior Probability:\n",
    "     - P(Y=1|X1=1, X2=1) = (P(X1=1|Y=1) * P(X2=1|Y=1) * P(Y=1)) / P(X1=1, X2=1)\n",
    "     - P(Y=0|X1=1, X2=1) = (P(X1=1|Y=0) * P(X2=1|Y=0) * P(Y=0)) / P(X1=1, X2=1)\n",
    "   \n",
    "   - We can now use the decision rule to choose the class with the highest posterior probability.\n",
    "   \n",
    "Let's calculate the probabilities using Bayes' theorem:\n",
    "\n",
    "Given:\n",
    "- P(X1=1|Y=1) = 2/3 ≈ 0.67\n",
    "- P(X2=1|Y=1) = 2/3 ≈ 0.67\n",
    "- P(X1=1|Y=0) = 0/2 = 0\n",
    "- P(X2=1|Y=0) = 1/2 = 0.5\n",
    "- P(Y=1) = 3/5\n",
    "- P(Y=0) = 2/5\n",
    "\n",
    "We need to calculate:\n",
    "- P(Y=1|X1=1, X2=1)\n",
    "- P(Y=0|X1=1, X2=1)\n",
    "\n",
    "Using Bayes' theorem:\n",
    "$$\\[ P(Y|X_1=1, X_2=1) = \\frac{P(X_1=1|Y) \\times P(X_2=1|Y) \\times P(Y)}{P(X_1=1, X_2=1)} \\]$$\n",
    "\n",
    "Let's compute each part step by step:\n",
    "\n",
    "1. **Calculate P(X1=1, X2=1)**:\n",
    "   - $$\\( P(X_1=1, X_2=1) = P(X_1=1, X_2=1|Y=1) \\times P(Y=1) + P(X_1=1, X_2=1|Y=0) \\times P(Y=0) \\)$$\n",
    "   - $$\\( P(X_1=1, X_2=1|Y=1) = \\frac{2}{3} \\times \\frac{2}{3} = \\frac{4}{9} \\)$$\n",
    "   - $$\\( P(X_1=1, X_2=1|Y=0) = 0 \\times \\frac{1}{2} = 0 \\)$$\n",
    "   - $$\\( P(X_1=1, X_2=1) = \\frac{4}{9} \\times \\frac{3}{5} + 0 \\times \\frac{2}{5} = \\frac{4}{15} \\)$$\n",
    "\n",
    "2. **Calculate P(Y=1|X1=1, X2=1)**:\n",
    "   - $$\\( P(Y=1|X_1=1, X_2=1) = \\frac{P(X_1=1|Y=1) \\times P(X_2=1|Y=1) \\times P(Y=1)}{P(X_1=1, X_2=1)} \\)$$\n",
    "   - $$\\( P(Y=1|X_1=1, X_2=1) = \\frac{\\frac{2}{3} \\times \\frac{2}{3} \\times \\frac{3}{5}}{\\frac{4}{15}} \\)$$\n",
    "   - $$\\( P(Y=1|X_1=1, X_2=1) = \\frac{\\frac{4}{9} \\times \\frac{3}{5}}{\\frac{4}{15}} \\)$$\n",
    "   - $$\\( P(Y=1|X_1=1, X_2=1) = \\frac{\\frac{12}{45}}{\\frac{4}{15}} \\)$$\n",
    "   - $$\\( P(Y=1|X_1=1, X_2=1) = \\frac{12}{45} \\times \\frac{15}{4} \\)$$\n",
    "   - $$\\( P(Y=1|X_1=1, X_2=1) = \\frac{1}{3} \\)$$\n",
    "\n",
    "3. **Calculate P(Y=0|X1=1, X2=1)**:\n",
    "   - $$\\( P(Y=0|X_1=1, X_2=1) = \\frac{P(X_1=1|Y=0) \\times P(X_2=1|Y=0) \\times P(Y=0)}{P(X_1=1, X_2=1)} \\)$$\n",
    "   - $$\\( P(Y=0|X_1=1, X_2=1) = \\frac{0 \\times \\frac{1}{2} \\times \\frac{2}{5}}{\\frac{4}{15}} \\)$$\n",
    "   - $$\\( P(Y=0|X_1=1, X_2=1) = 0 \\)$$\n",
    "\n",
    "So, the final results are:\n",
    "- $$\\( P(Y=1|X_1=1, X_2=1) = \\frac{1}{3} \\)$$\n",
    "- $$\\( P(Y=0|X_1=1, X_2=1) = 0 \\)$$\n",
    "\n",
    "These values represent the probability that an email is classified as spam or not spam given that it contains \"free\" and \"buy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0affa4e-96d5-43e5-915e-75747c8f5500",
   "metadata": {},
   "source": [
    "### Refer this Pdf\n",
    "\n",
    "[PDF](05_naive_bayes_intuition_handwritten.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
