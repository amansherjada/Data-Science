{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd72041-717e-4e60-9393-6b253a70699f",
   "metadata": {},
   "source": [
    "### sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "```\n",
    "class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, \n",
    "                                              criterion='gini', \n",
    "                                              max_depth=None, \n",
    "                                              min_samples_split=2, \n",
    "                                              min_samples_leaf=1, \n",
    "                                              min_weight_fraction_leaf=0.0, \n",
    "                                              max_features='sqrt', \n",
    "                                              max_leaf_nodes=None, \n",
    "                                              min_impurity_decrease=0.0, \n",
    "                                              bootstrap=True, \n",
    "                                              oob_score=False, \n",
    "                                              n_jobs=None, \n",
    "                                              random_state=None, \n",
    "                                              verbose=0, \n",
    "                                              warm_start=False, \n",
    "                                              class_weight=None, \n",
    "                                              ccp_alpha=0.0, \n",
    "                                              max_samples=None, \n",
    "                                              monotonic_cst=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb722b4-f470-4d40-8bce-128677c3601c",
   "metadata": {},
   "source": [
    "Let's break down each hyperparameter of the `RandomForestClassifier` class from scikit-learn:\n",
    "\n",
    "1. **n_estimators**:\n",
    "   - Definition: The number of trees in the forest.\n",
    "   - Explanation: Determines how many decision trees will be built in the forest. More trees can potentially lead to better performance but also increase computational cost.\n",
    "\n",
    "2. **criterion**:\n",
    "   - Definition: The function to measure the quality of a split.\n",
    "   - Explanation: Specifies the criterion used to measure the quality of a split when building each tree. It can be either 'gini' for the Gini impurity or 'entropy' for the information gain.\n",
    "\n",
    "3. **max_depth**:\n",
    "   - Definition: The maximum depth of the tree.\n",
    "   - Explanation: Limits the maximum depth of each decision tree in the forest. Constraining the depth helps prevent overfitting by restricting the tree's complexity.\n",
    "\n",
    "4. **min_samples_split**:\n",
    "   - Definition: The minimum number of samples required to split an internal node.\n",
    "   - Explanation: Specifies the minimum number of samples required to split an internal node during tree construction. It prevents splitting nodes that have too few samples, which can lead to overfitting.\n",
    "\n",
    "5. **min_samples_leaf**:\n",
    "   - Definition: The minimum number of samples required to be at a leaf node.\n",
    "   - Explanation: Sets the minimum number of samples required to be at a leaf node. It ensures that each leaf node has a minimum number of samples, which can help prevent overfitting and improve generalization.\n",
    "\n",
    "6. **min_weight_fraction_leaf**:\n",
    "   - Definition: The minimum weighted fraction of the sum total of weights (of the input samples) required to be at a leaf node.\n",
    "   - Explanation: Similar to `min_samples_leaf`, but expressed as a fraction of the total number of weighted samples. Useful when working with weighted data.\n",
    "\n",
    "7. **max_features**:\n",
    "   - Definition: The number of features to consider when looking for the best split.\n",
    "   - Explanation: Specifies the number of features to consider when looking for the best split. It can be an integer representing the number of features or a string representing a percentage of features. 'sqrt' means the square root of the total number of features, and 'log2' means the log base 2 of the total number of features.\n",
    "\n",
    "8. **max_leaf_nodes**:\n",
    "   - Definition: The maximum number of leaf nodes in each tree.\n",
    "   - Explanation: Limits the maximum number of leaf nodes in each decision tree. It helps control the growth of the tree and prevents overfitting.\n",
    "\n",
    "9. **min_impurity_decrease**:\n",
    "   - Definition: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "   - Explanation: Specifies the minimum impurity decrease required to split a node. It prevents splitting nodes that don't sufficiently reduce impurity, which can help prevent overfitting.\n",
    "\n",
    "10. **bootstrap**:\n",
    "    - Definition: Whether bootstrap samples are used when building trees.\n",
    "    - Explanation: Indicates whether bootstrap samples (sampling with replacement) are used to train individual trees. Bootstrap sampling is commonly used to introduce randomness and improve generalization.\n",
    "\n",
    "11. **oob_score**:\n",
    "    - Definition: Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "    - Explanation: Determines whether to use out-of-bag (OOB) samples to estimate the model's accuracy. OOB samples are data points that are not included in the bootstrap sample used to train each tree. Using OOB samples can provide an unbiased estimate of the model's performance without the need for a separate validation set.\n",
    "\n",
    "12. **n_jobs**:\n",
    "    - Definition: The number of jobs to run in parallel for both fit and predict.\n",
    "    - Explanation: Specifies the number of parallel jobs to run during model training and prediction. Setting it to -1 uses all available CPU cores.\n",
    "\n",
    "13. **random_state**:\n",
    "    - Definition: Controls the randomness of the estimator.\n",
    "    - Explanation: Sets the seed used by the random number generator. Providing a fixed value ensures reproducibility of results across multiple runs.\n",
    "\n",
    "14. **verbose**:\n",
    "    - Definition: Controls the verbosity when fitting and predicting.\n",
    "    - Explanation: Controls the amount of logging output during the model fitting and prediction process. Higher values result in more verbose output.\n",
    "\n",
    "15. **warm_start**:\n",
    "    - Definition: When set to True, reuses the solution of the previous call to fit and adds more estimators to the ensemble.\n",
    "    - Explanation: Allows incremental training of the Random Forest model. If set to True, the model continues training from the existing solution rather than starting from scratch.\n",
    "\n",
    "16. **class_weight**:\n",
    "    - Definition: Weights associated with classes in the form {class_label: weight}.\n",
    "    - Explanation: Allows you to specify weights for different classes in the dataset. Useful for handling imbalanced datasets by giving more weight to minority classes.\n",
    "\n",
    "17. **ccp_alpha**:\n",
    "    - Definition: Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "    - Explanation: Controls the amount of regularization applied to the tree by pruning. Higher values result in more aggressive pruning, leading to simpler trees.\n",
    "\n",
    "18. **max_samples**:\n",
    "    - Definition: The maximum number of samples to draw from X to train each base estimator.\n",
    "    - Explanation: Specifies the maximum number of samples to draw from the dataset to train each tree. Useful for building Random Forests on subsets of the data, particularly in cases of very large datasets.\n",
    "\n",
    "19. **monotonic_cst**:\n",
    "    - Definition: Monotonic constraints to respect for each feature. If not None, it must be an array of shape (n_features,) containing the constants.\n",
    "    - Explanation: Specifies monotonic constraints that the model must respect for each feature. For example, it can enforce that the relationship between a feature and the target variable is strictly increasing or decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7477f-5876-48d0-9a52-bdeafd1ce8da",
   "metadata": {},
   "source": [
    "Documentation = https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Documentation = https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
