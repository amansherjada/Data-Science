{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e82e43-07f4-46a0-b04d-bfa218ef4ee0",
   "metadata": {},
   "source": [
    "Documentation = https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e4690-0ba2-4b1d-9bba-e6c8760a1181",
   "metadata": {},
   "source": [
    "```python\n",
    "class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "```\n",
    "\n",
    "Let's break down each parameter of the `LogisticRegression` class from scikit-learn and explain its purpose:\n",
    "\n",
    "1. `penalty`: This parameter specifies the type of regularization used in the logistic regression model. It can take one of the following values:\n",
    "   - `'l1'`: L1 regularization, also known as Lasso regularization, which adds a penalty term to the absolute values of the coefficients.\n",
    "   - `'l2'`: L2 regularization, also known as Ridge regularization, which adds a penalty term to the squared magnitudes of the coefficients.\n",
    "   - `'none'`: No regularization is applied.\n",
    "\n",
    "2. `dual`: This parameter is used only when `solver` is set to `'liblinear'`. It determines whether to solve the primal or dual optimization problem. Setting it to `False` solves the primal problem.\n",
    "\n",
    "3. `tol`: This parameter sets the tolerance for the optimization algorithm. It determines the stopping criterion for the optimization process. The optimization process stops when the change in the objective function between iterations is less than this tolerance value.\n",
    "\n",
    "4. `C`: This parameter controls the regularization strength. A smaller value of `C` indicates stronger regularization, meaning the model will be less likely to overfit the training data.\n",
    "\n",
    "5. `fit_intercept`: This parameter determines whether to calculate the intercept for the logistic regression model. If set to `True`, an intercept term will be included in the model.\n",
    "\n",
    "6. `intercept_scaling`: This parameter scales the intercept term. It is useful when regularization is applied and the intercept needs to be penalized differently from the other coefficients.\n",
    "\n",
    "7. `class_weight`: This parameter allows you to assign weights to different classes. It can be useful for imbalanced classification problems, where some classes have more instances than others.\n",
    "\n",
    "8. `random_state`: This parameter sets the random seed used for initializing the logistic regression solver. It ensures reproducibility of results across multiple runs.\n",
    "\n",
    "9. `solver`: This parameter specifies the optimization algorithm used to solve the logistic regression problem. It can take one of the following values:\n",
    "   - `'lbfgs'`: Limited-memory BFGS algorithm, suitable for small and medium-sized datasets.\n",
    "   - `'liblinear'`: Coordinate descent algorithm, suitable for large datasets.\n",
    "   - `'newton-cg'`: Newton Conjugate Gradient algorithm, suitable for multiclass problems.\n",
    "   - `'sag'`: Stochastic Average Gradient algorithm, suitable for large datasets.\n",
    "   - `'saga'`: Improved version of SAG algorithm.\n",
    "\n",
    "Warning The choice of the algorithm depends on the penalty chosen. Supported penalties by solver:\n",
    "‘lbfgs’ - [‘l2’, None]\n",
    "\n",
    "‘liblinear’ - [‘l1’, ‘l2’]\n",
    "\n",
    "‘newton-cg’ - [‘l2’, None]\n",
    "\n",
    "‘newton-cholesky’ - [‘l2’, None]\n",
    "\n",
    "‘sag’ - [‘l2’, None]\n",
    "\n",
    "‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, None]\n",
    "\n",
    "\n",
    "10. `max_iter`: This parameter specifies the maximum number of iterations for the optimization algorithm. It prevents the algorithm from running indefinitely and helps control computational resources.\n",
    "\n",
    "11. `multi_class`: This parameter determines how to handle multiclass classification problems. It can take one of the following values:\n",
    "   - `'auto'`: Automatically selects the appropriate strategy based on the nature of the problem.\n",
    "   - `'ovr'`: One-vs-Rest strategy, also known as one-vs-all.\n",
    "   - `'multinomial'`: Multinomial logistic regression strategy.\n",
    "\n",
    "12. `verbose`: This parameter controls the verbosity of the optimization process. A higher value results in more detailed output during optimization.\n",
    "\n",
    "13. `warm_start`: This parameter allows you to reuse the solution of the previous call to `fit` as the initial guess for the next call. It can be useful for incremental training.\n",
    "\n",
    "14. `n_jobs`: This parameter specifies the number of CPU cores to use for parallel computation. Setting it to `-1` utilizes all available CPU cores.\n",
    "\n",
    "15. `l1_ratio`: This parameter is used only when `penalty` is set to `'elasticnet'`. It controls the mix of L1 and L2 regularization. A value of `0` corresponds to pure L2 regularization, while a value of `1` corresponds to pure L1 regularization. Values in between indicate a mixture of both.\n",
    "\n",
    "These parameters give you control over various aspects of the logistic regression model, such as regularization, optimization, handling of multiclass problems, and more, allowing you to customize the model according to your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e255c9-e39c-4887-939c-0d5fa67ff609",
   "metadata": {},
   "source": [
    "\r\n",
    "| Parameter          | Description                                                                                       | Supported Penalties                      |\r\n",
    "|--------------------|---------------------------------------------------------------------------------------------------|------------------------------------------|\r\n",
    "| `penalty`          | Type of regularization used in the logistic regression model                                      | `'l2'`, `None`                           |\r\n",
    "| `dual`             | Whether to solve the primal or dual optimization problem                                           | N/A                                      |\r\n",
    "| `tol`              | Tolerance for the optimization algorithm                                                          | N/A                                      |\r\n",
    "| `C`                | Regularization strength                                                                            | N/A                                      |\r\n",
    "| `fit_intercept`    | Whether to calculate the intercept for the logistic regression model                              | N/A                                      |\r\n",
    "| `intercept_scaling`| Scaling factor for the intercept term                                                             | N/A                                      |\r\n",
    "| `class_weight`     | Weights assigned to different classes                                                              | N/A                                      |\r\n",
    "| `random_state`     | Random seed for initializing the logistic regression solver                                        | N/A                                      |\r\n",
    "| `solver`           | Optimization algorithm used to solve the logistic regression problem                               | See below                                |\r\n",
    "| `max_iter`         | Maximum number of iterations for the optimization algorithm                                       | N/A                                      |\r\n",
    "| `multi_class`      | Strategy for handling multiclass problems                                                         | `'l2'`, `None`                           |\r\n",
    "| `verbose`          | Verbosity of the optimization process                                                              | N/A                                      |\r\n",
    "| `warm_start`       | Whether to reuse the solution of the previous call to `fit` as the initial guess for the next call | N/A                                      |\r\n",
    "| `n_jobs`           | Number of CPU cores to use for parallel computation                                                | N/A                                      |\r\n",
    "| `l1_ratio`         | Mix of L1 and L2 regularization (used only with `penalty='elasticnet'`)                            | `'elasticnet'`, `'l1'`, `'l2'`, `None` |\r\n",
    "\r\n",
    "Supported Penalties by Solver:\r\n",
    "\r\n",
    "- `'lbfgs'`: `['l2', None]`\r\n",
    "- `'liblinear'`: `['l1', 'l2']`\r\n",
    "- `'newton-cg'`: `['l2', None]`\r\n",
    "- `'newton-cholesky'`: `['l2', None]`\r\n",
    "- `'sag'`: `['l2', None]`\r\n",
    "- `'saga'`: `['elasticnet', 'l1', 'l2', None]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
