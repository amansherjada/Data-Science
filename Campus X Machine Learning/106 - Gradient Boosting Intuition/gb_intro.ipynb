{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e044f0d-110f-42e2-bccd-fac04fc95065",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "![gb_gif](https://miro.medium.com/v2/resize:fit:1400/1*KgL4w8IGOSc_xBS1KEIPaQ.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3f2c6-b586-47a3-91fa-36bdb6d4a1da",
   "metadata": {},
   "source": [
    "Gradient Boosting is a machine learning ensemble technique used for both regression and classification tasks. It builds predictive models in the form of an ensemble of weak learners (usually decision trees), with each subsequent model correcting the errors of the previous ones. Here's a simple definition:\r\n",
    "\r\n",
    "**Gradient Boosting**:\r\n",
    "Gradient Boosting is an iterative ensemble learning technique that sequentially trains a series of weak learners, typically decision trees, to minimize a loss function by optimizing the gradient of the loss function. Each weak learner is trained to predict the residuals (the differences between the actual target values and the predictions of the previous model) in order to gradually improve the overall prediction accuracy of the ensemble.\r\n",
    "\r\n",
    "In simpler terms:\r\n",
    "\r\n",
    "- **Ensemble Learning**: Gradient Boosting combines multiple weak models (typically decision trees) to create a stronger predictive model.\r\n",
    "- **Iterative Process**: It trains a series of models sequentially, with each subsequent model focusing on improving the errors made by the previous ones.\r\n",
    "- **Gradient Optimization**: It minimizes a loss function by optimizing the gradient of the loss function. This means it adjusts the predictions of each model in the ensemble based on how much they contribute to reducing the overall error.\r\n",
    "- **Residuals**: Each model in the sequence is trained to predict the residuals (the differences between the actual target values and the predictions of the previous model) rather than directly predicting the target values.\r\n",
    "- **Improved Accuracy**: By combining the predictions of multiple weak learners and iteratively refining them, Gradient Boosting creates a highly accurate predictive model.\r\n",
    "\r\n",
    "Overall, Gradient Boosting is a powerful technique known for its ability to produce highly accurate predictions, making it widely used in various machine learning tasks. Popular implementations include XGBoost, LightGBM, and CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163f7516-df23-4e66-b522-64e7d90d2f75",
   "metadata": {},
   "source": [
    "### Gradient Boosting using a example involving predicting house prices.\n",
    "\n",
    "Imagine you're trying to predict the prices of houses based on features like size, number of bedrooms, and location. Here's how Gradient Boosting might work in this scenario:\n",
    "\n",
    "1. **Initial Prediction**:\n",
    "   - You start with an initial prediction for house prices. This could be a simple average of all house prices in your dataset.\n",
    "\n",
    "2. **First Weak Learner (Decision Tree)**:\n",
    "   - You train a weak learner, such as a decision tree, to predict house prices based on the features.\n",
    "   - This decision tree might make some incorrect predictions, either overestimating or underestimating the prices.\n",
    "\n",
    "3. **Residual Calculation**:\n",
    "   - Calculate the residuals, which are the differences between the actual house prices and the predictions made by the decision tree.\n",
    "   - These residuals represent the errors or the parts of the predictions that the decision tree got wrong.\n",
    "\n",
    "4. **Second Weak Learner**:\n",
    "   - Train another decision tree, but this time, instead of predicting the house prices directly, it predicts the residuals from the first decision tree.\n",
    "   - This second tree focuses on correcting the errors made by the first tree.\n",
    "\n",
    "5. **Update Predictions**:\n",
    "   - Combine the predictions from the first and second trees to get an updated set of predictions for house prices.\n",
    "   - This updated prediction will be closer to the actual prices because it incorporates the corrections made by the second tree.\n",
    "\n",
    "6. **Repeat**:\n",
    "   - Repeat steps 3 to 5 for a predefined number of iterations or until a stopping criterion is met.\n",
    "   - Each iteration introduces a new weak learner that focuses on correcting the errors of the ensemble from the previous iterations.\n",
    "\n",
    "7. **Final Prediction**:\n",
    "   - After several iterations, you have an ensemble of weak learners (decision trees) that collectively make predictions for house prices.\n",
    "   - The final prediction is the sum of the initial prediction and the predictions made by each weak learner.\n",
    "\n",
    "Here's a simplified example:\n",
    "\n",
    "- Initial Prediction: $200,000\n",
    "\n",
    "- First Weak Learner predicts: $180,000\n",
    "\n",
    "- Residual: $20,000\n",
    "\n",
    "- Second Weak Learner predicts: $15,000 (corrects the residual)\n",
    "\n",
    "- Updated Prediction: $195,000\n",
    "\n",
    "- Final Prediction: $200,000 + $195,000 = $395,000\n",
    "\n",
    "In this example, Gradient Boosting sequentially corrects the errors made by weak learners, gradually improving the prediction accuracy of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e6942-c1a2-43ef-9eb6-d4c5b85b08cc",
   "metadata": {},
   "source": [
    "Video = https://youtu.be/fbKz7N92mhQ?si=DeS8wHCxQceUIsZp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1555e2e0-8b2c-4f1d-ac56-9f20d91228b8",
   "metadata": {},
   "source": [
    "### Gradient Boosting in detail\n",
    "\n",
    "**Gradient Boosting**:\n",
    "Gradient Boosting is an ensemble technique that combines multiple weak learners, usually decision trees, to create a strong predictive model. It builds the model in a stage-wise fashion, where each new model in the sequence corrects the errors made by the previous ones.\n",
    "\n",
    "**Key Components**:\n",
    "1. **Weak Learners**: These are simple models, typically decision trees, that perform slightly better than random guessing.\n",
    "2. **Learning Rate**: This is a hyperparameter that controls the contribution of each weak learner to the overall prediction. A smaller learning rate requires more iterations but often leads to better performance and more stable models.\n",
    "\n",
    "**Gradient Boosting Process**:\n",
    "Let's break down the Gradient Boosting process with a learning rate, considering three weak learners:\n",
    "\n",
    "1. **First Model**:\n",
    "   - We start with an initial prediction, often the mean of the target variable.\n",
    "   - The first weak learner (decision tree) is trained to predict the residuals (errors) between the initial prediction and the true target values.\n",
    "   - The model aims to minimize the loss function (e.g., mean squared error) by optimizing the gradient of the loss function with respect to the predictions.\n",
    "   - Formula for the first model's prediction\n",
    "   - `ùë¶ÃÇ‚ÇÅ = ùëì‚ÇÅ(ùë•)`\n",
    "\n",
    "2. **Second Model**:\n",
    "   - The second weak learner is trained to predict the residuals from the first model.\n",
    "   - It focuses on correcting the errors (residuals) made by the first model.\n",
    "   - Formula for the second model's prediction:\n",
    "   - `ùë¶ÃÇ‚ÇÇ = ùëì‚ÇÇ(ùë•)`\n",
    "   - Updated prediction after the second model:\n",
    "   - `ùë¶ÃÇ_{updated} = ùë¶ÃÇ‚ÇÅ + ùúÜ ‚ãÖ ùë¶ÃÇ‚ÇÇ`\n",
    "   \n",
    "   Where:\n",
    "     - ùë¶ÃÇ‚ÇÅ is the prediction from the first model.\n",
    "     - ùë¶ÃÇ‚ÇÇ is the prediction from the second model.\n",
    "     - ùúÜ is the learning rate.\n",
    "\n",
    "3. **Third Model**:\n",
    "   - Similarly, the third weak learner is trained to predict the residuals from the updated prediction after the second model.\n",
    "   - It further refines the predictions by focusing on correcting the errors remaining after the first two models.\n",
    "   - Formula for the third model's prediction:\n",
    "   - `ùë¶ÃÇ‚ÇÉ = ùëì‚ÇÉ(ùë•)`\n",
    "   - Updated prediction after the third model:\n",
    "   - `ùë¶ÃÇ‚Çñ = ùë¶ÃÇ·µ§‚Çñ + Œª ‚ãÖ ùë¶ÃÇ‚ÇÉ`\n",
    "\n",
    "**Repeat**:\n",
    "- This process can continue with more weak learners, each aiming to correct the residuals from the previous ensemble of models.\n",
    "- The predictions from all weak learners are combined with a scaled learning rate to produce the final prediction.\n",
    "\n",
    "In summary, Gradient Boosting with a learning rate involves iteratively training weak learners to correct the errors made by the previous models, gradually improving the overall prediction accuracy. The learning rate controls the impact of each model on the final prediction, allowing for fine-tuning of the boosting process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
