{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e54c42-a960-43c1-b5c2-fa9e15e956a5",
   "metadata": {},
   "source": [
    "### Decision Tree Hyperparameters\n",
    "\n",
    "```\n",
    "class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None)\n",
    "```\n",
    "\n",
    "\r\n",
    "1. **criterion**:\r\n",
    "   - This parameter specifies the function to measure the quality of a split. It can be either 'gini' for Gini impurity or 'entropy' for information gain. Default is 'gini'.\r\n",
    "\r\n",
    "2. **splitter**:\r\n",
    "   - This parameter specifies the strategy used to choose the split at each node. It can be 'best' to choose the best split or 'random' to choose the best random split. Default is 'best'.\r\n",
    "\r\n",
    "3. **max_depth**:\r\n",
    "   - This parameter specifies the maximum depth of the tree. If None, nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. Default is None.\r\n",
    "\r\n",
    "4. **min_samples_split**:\r\n",
    "   - The minimum number of samples required to split an internal node. Default is 2.\r\n",
    "\r\n",
    "5. **min_samples_leaf**:\r\n",
    "   - The minimum number of samples required to be at a leaf node. Default is 1.\r\n",
    "\r\n",
    "6. **min_weight_fraction_leaf**:\r\n",
    "   - The minimum weighted fraction of the sum total of weights (of all input samples) required to be at a leaf node. Default is 0.0.\r\n",
    "\r\n",
    "7. **max_features**:\r\n",
    "   - The number of features to consider when looking for the best split. It can be an int, float, 'auto', 'sqrt', 'log2', None, or a fraction of features. Default is None.\r\n",
    "\r\n",
    "8. **random_state**:\r\n",
    "   - This parameter is used to control the random number generator. It can be an int or an instance of np.random.RandomState. Default is None.\r\n",
    "\r\n",
    "9. **max_leaf_nodes**:\r\n",
    "   - The maximum number of leaf nodes in the tree. Default is None.\r\n",
    "\r\n",
    "10. **min_impurity_decrease**:\r\n",
    "    - A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Default is 0.0.\r\n",
    "\r\n",
    "11. **class_weight**:\r\n",
    "    - This parameter is used to specify the weight associated with classes. It can be a dictionary, 'balanced', 'balanced_subsample', or None. Default is None.\r\n",
    "\r\n",
    "12. **ccp_alpha**:\r\n",
    "    - Complexity parameter used for Minimal Cost-Complexity Pruning. Default is 0.0.\r\n",
    "\r\n",
    "13. **monotonic_cst**:\r\n",
    "    - This parameter is used to specify monotonic constraints for the decision tree. Default is None.\r\n",
    "\r\n",
    "Each hyperparameter provides a way to control the behavior and performance of the decision tree classifier. By adjusting these parameters, you can tailor the decision tree model to suit the specific characteristics of your dataset and the requirements of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997dd189-e3f4-428d-8522-ae07ce305ecc",
   "metadata": {},
   "source": [
    "\r\n",
    "| Hyperparameter            | Explanation                                                                                       |\r\n",
    "|---------------------------|---------------------------------------------------------------------------------------------------|\r\n",
    "| criterion                 | Criterion used to measure the quality of a split: 'gini' for Gini impurity or 'entropy' for information gain.                                         |\r\n",
    "| splitter                  | Strategy used to choose the split at each node: 'best' to choose the best split or 'random' to choose the best random split.                        |\r\n",
    "| max_depth                 | Maximum depth of the tree.                                                                       |\r\n",
    "| min_samples_split         | Minimum number of samples required to split an internal node.                                     |\r\n",
    "| min_samples_leaf          | Minimum number of samples required to be at a leaf node.                                           |\r\n",
    "| min_weight_fraction_leaf  | Minimum weighted fraction of the sum total of weights required to be at a leaf node.             |\r\n",
    "| max_features              | Number of features to consider when looking for the best split.                                    |\r\n",
    "| random_state              | Seed used by the random number generator.                                                         |\r\n",
    "| max_leaf_nodes            | Maximum number of leaf nodes in the tree.                                                         |\r\n",
    "| min_impurity_decrease     | Minimum impurity decrease required for a split to happen.                                          |\r\n",
    "| class_weight              | Weight associated with classes to address class imbalance.                                         |\r\n",
    "| ccp_alpha                 | Complexity parameter used for Minimal Cost-Complexity Pruning.                                      |\r\n",
    "| monotonic_cst             | Monotonic constraints for the decision tree.                                                      |\r\n",
    "\r\n",
    "These hyperparameters control various aspects of the decision tree model, such as its depth, the criteria for splitting, and how it handles imbalanced classes. Adjusting these parameters allows you to fine-tune the model's performance based on the characteristics of your dataset and the requirements of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af0c7c-a4ba-4f38-ae99-17aa55eebf22",
   "metadata": {},
   "source": [
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
